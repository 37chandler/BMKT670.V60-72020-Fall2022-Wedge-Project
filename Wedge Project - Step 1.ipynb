{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bf0d7e",
   "metadata": {},
   "source": [
    "# The Wedge\n",
    "\n",
    "## Task 1: Building a Transaction Database in Google Big Query!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19c439",
   "metadata": {},
   "source": [
    "## Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d8424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import re\n",
    "import datetime \n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import janitor\n",
    "\n",
    "from zipfile import ZipFile # usually you'd do all these imports at the beginning\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Python program to illustrate the concept\n",
    "# of threading\n",
    "# importing the threading module\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee09c70",
   "metadata": {},
   "source": [
    "## Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cf01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Small File Sample\n",
    "# zip_file_name = \"WedgeZipOfZips_Small.zip\"\n",
    "\n",
    "## Full data Set\n",
    "zip_file_name = \"WedgeZipOfZips.zip\"\n",
    "\n",
    "# Clean data Set\n",
    "# zip_file_name = \"WedgeFiles_Clean.zip\"\n",
    "\n",
    "# Small Clean Data Set\n",
    "# zip_file_name = \"WedgeZipOfZips_Small_Clean.zip\"\n",
    "\n",
    "# Working Directory included in .gitignore\n",
    "# working_directory = \"/media/psf/Home/Repos/BMKT670.V60-72020-Fall2022-Wedge-Project/eggs/\"\n",
    "working_directory = \"/home/blackvwgolf95/BMKT670.V60-72020-Fall2022-Wedge-Project/eggs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6314070",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4479d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(zf):\n",
    "    # printing what's in the zip file.  \n",
    "    # zf.printdir() \n",
    "\n",
    "    # extracting all the files \n",
    "    print('Extracting all the files now...') \n",
    "    # pick a folder name already in .gitignore\n",
    "    \n",
    "    # Instead of always extracting ALL, check if file exists first\n",
    "    # zf.extractall(working_directory) \n",
    "    \n",
    "    zipped_files = zf.namelist()\n",
    "    # display_zip_contents(zipped_files)\n",
    "    \n",
    "    # Only extract files if they don't exist\n",
    "    for file_name in zipped_files :\n",
    "        \n",
    "        # Ignore .DS_Store hidden files\n",
    "        if(file_name.endswith( '.DS_Store' )):\n",
    "            continue\n",
    "            \n",
    "        # Ignore __MACOSX hidden files\n",
    "        if(file_name.startswith( '__' )):\n",
    "            continue\n",
    "        \n",
    "        # Ignore folders\n",
    "        if(file_name.endswith( '/' )):\n",
    "            continue\n",
    "        \n",
    "        if os.path.exists(working_directory + file_name) :\n",
    "            print(\"File Exists, skipping\")\n",
    "            print(file_name)\n",
    "        else :\n",
    "            print(\"Need to Extract\")\n",
    "            print(file_name)\n",
    "            zf.extract(file_name, working_directory) \n",
    "        \n",
    "        zip_files.append(file_name)\n",
    "        \n",
    "def extract_single_zip(zf):\n",
    "    zipped_files = zf.namelist()\n",
    "    # display_zip_contents(zipped_files)\n",
    "    \n",
    "    # Only extract files if they don't exist\n",
    "    for file_name in zipped_files :\n",
    "        \n",
    "        # Ignore .DS_Store hidden files\n",
    "        if(file_name.endswith( '.DS_Store' )):\n",
    "            continue\n",
    "            \n",
    "        # Ignore __MACOSX hidden files\n",
    "        if(file_name.startswith( '__' )):\n",
    "            continue\n",
    "        \n",
    "        # Ignore folders\n",
    "        if(file_name.endswith( '/' )):\n",
    "            continue\n",
    "        \n",
    "        if os.path.exists(working_directory + file_name) :\n",
    "            print(\"File Exists, skipping\")\n",
    "            print(file_name)\n",
    "        else :\n",
    "            print(\"Need to Extract\")\n",
    "            print(file_name)\n",
    "            zf.extract(file_name, working_directory) \n",
    "        \n",
    "        data_files.append(file_name)\n",
    "\n",
    "\n",
    "def display_zip_contents(zipped_files):\n",
    "    for file_name in zipped_files :\n",
    "        # Ignore __MACOSX hidden files\n",
    "        if(file_name.startswith( '__' )):\n",
    "            continue\n",
    "        # Ignore .DS_Store hidden files\n",
    "        if(file_name.endswith( '.DS_Store' )):\n",
    "            continue\n",
    "        # Ignore folders\n",
    "        if(file_name.endswith( '/' )):\n",
    "            continue\n",
    "\n",
    "        print(\"File: \", file_name,\" Size:\", os.path.getsize(working_directory+file_name))\n",
    "\n",
    "def display_file_contents(files):\n",
    "    for file_name in files :\n",
    "        # Ignore __MACOSX hidden files\n",
    "        if(file_name.startswith( '__' )):\n",
    "            continue\n",
    "        # Ignore .DS_Store hidden files\n",
    "        if(file_name.endswith( '.DS_Store' )):\n",
    "            continue\n",
    "        # Ignore folders\n",
    "        if(file_name.endswith( '/' )):\n",
    "            continue\n",
    "\n",
    "        print(\"File: \", file_name,\" Size:\", os.path.getsize(working_directory+file_name))\n",
    "\n",
    "def get_delimiter(file_name) :\n",
    "    # Get separator\n",
    "    input_file = open(working_directory+file_name,'r')\n",
    "    # input_file = io.TextIOWrapper(input_file,encoding=\"utf-8\")\n",
    "            \n",
    "    dialect = csv.Sniffer().sniff(sample=input_file.readline(),\n",
    "                                  delimiters=[\",\",\";\",\"\\t\"])\n",
    "    delimiter = dialect.delimiter\n",
    "    # delimiters[file_name] = dialect.delimiter\n",
    "\n",
    "    #     print(\" \".join([\"It looks like\",\n",
    "    #                    file_name,\n",
    "    #                    \"has delimiter\",\n",
    "    #                    dialect.delimiter,\n",
    "    #                    \".\"]))\n",
    "    input_file.close() # tidy up\n",
    "    return delimiter\n",
    "\n",
    "def get_header(file_name) :\n",
    "    \n",
    "    with open(working_directory+file_name) as f:\n",
    "        first_line = f.readline()\n",
    "        # print(first_line)\n",
    "        if first_line.startswith('datetime') :\n",
    "            return 0\n",
    "        if first_line.startswith('\"datetime\"') :\n",
    "            return 0\n",
    "        if first_line.startswith(\"'datetime'\") :\n",
    "            return 0\n",
    "        else :\n",
    "            return None\n",
    "\n",
    "def upload_data(data):\n",
    "    # https://stackoverflow.com/a/24083253\n",
    "    grouped = data.groupby(pd.Grouper(freq='M'))\n",
    "    for name, group in grouped:\n",
    "\n",
    "        # Construct table name from index\n",
    "        # table_name = \"dram_items_\"+reformat_date(name.strftime('%Y-%m-%d'))\n",
    "\n",
    "        # 3. For each month in the file, subset the data to that month and \n",
    "        #    upload the data to a table called `dram_items_YYYYMM01`. \n",
    "        # table_id = \".\".join([gbq_proj_id,dataset_id,table_name])\n",
    "        # print(table_id)\n",
    "        # pandas_gbq.to_gbq(item_lu, table_id, project_id=gbq_proj_id,if_exists=\"replace\") # let's discuss this last bit\n",
    "        print(\"Data Uploaded!\")\n",
    "        \n",
    "def cleanup_data(data): \n",
    "    # Clean the names with the janitor package.\n",
    "    data = janitor.clean_names(data)\n",
    "\n",
    "#     for column in ( 'gross_sales', 'discounts', 'net_sales', 'tax' ):\n",
    "#         # Convert the fields that have dollar signs (such as `gross_sales`) into numeric data. Watch out for dollar signs and commas.\n",
    "#         data[column] = ( data[column]\n",
    "#                            .str.replace(\"$\", '', regex=False)\n",
    "#                            .str.replace(\",\", '', regex=False)\n",
    "#                            .astype(float) )\n",
    "\n",
    "    # Change the type of the column `modifiers_applied` to string.\n",
    "#     data['modifiers_applied'] = data['modifiers_applied'].astype(str)\n",
    "\n",
    "    # Replace the `sku` column with a column of empty strings. \n",
    "#     data['sku'] = ''\n",
    "\n",
    "    # print( item_lu.head() )\n",
    "#     data.index = pd.to_datetime(data['date']) # ,format='%y-%m-%d'  \n",
    "    return data\n",
    "\n",
    "\n",
    "def process_file(file_name):\n",
    "    # print(file_name)\n",
    "    data = pd.read_csv(working_directory+file_name, low_memory=False)\n",
    "  \n",
    "    # 2. Do the same cleaning we did in Part 1 (clean names, \n",
    "    #    make sku an empty string, fix dollars, make modifiers_applied a string)\n",
    "    data = cleanup_data(data)\n",
    "    upload_data(data)\n",
    "\n",
    "\n",
    "def data_columns():\n",
    "    # https://www.geeksforgeeks.org/add-column-names-to-dataframe-in-pandas/\n",
    "    columns = []\n",
    "    # 1.\tdatetime: timestamp of the transaction-row creation   \n",
    "    columns.append('datetime') # \n",
    "    # 2.\tregister_no: register for transaction\n",
    "    columns.append('register_no') # \n",
    "    # 3.\temp_no: employee number for cashier   \n",
    "    columns.append('emp_no') # \n",
    "    # 4.\ttrans_no: transaction number. This number counts up by day and is only unique when combined with date, columns.append('trans_no') # register and employee.\n",
    "    columns.append('trans_no')\n",
    "    # 5.\tUpc: Universal Product Code for the item. 0 for non-items.\n",
    "    columns.append('Upc') # \n",
    "    # 6.\tdescription: product description. Includes things like Tax, Tender type, etc.   \n",
    "    columns.append('description') # \n",
    "    # 7.\ttrans_type: One of five values (D, G, A, T, and I). These correspond to the following types of columns.append('trans_type') # transactions:\n",
    "    # •\tD: Departmental rings, when the cashier just selects a department for the item.\n",
    "    # •\tG: Green patch donations. This is the donation made for shoppers who bring their own bag. \n",
    "    # •\tA: Tax\n",
    "    # •\tT: Tender, the payment row.\n",
    "    # •\tI: Items, but also includes discounts. \n",
    "    columns.append('trans_type')\n",
    "    # 8.\ttrans_subtype: There are a lot of these. Key ones include methods of payment (CK for Check, CA for columns.append('trans_subtype') # Cash, CP for coupon, EF for EBT Food Stamps , WC for WIC). These are often blank for other trans_type values.\n",
    "    columns.append('trans_subtype')\n",
    "    # 9.\ttrans_status: An important field. The field trans_status tells us more about the types transactions. columns.append('trans_status') # Here are the possible values:\n",
    "    # •\tBlank: The typical value.\n",
    "    # •\tM: Member discounts.\n",
    "    # •\tV: Voids \n",
    "    # •\tC: Coupons\n",
    "    # •\t0: Honestly, I think these are supposed to be blanks but they changed from 0s at some point in February 2010. \n",
    "    # •\tR: Returns.\n",
    "    # •\tJ: Juice club cards\n",
    "    columns.append('trans_status')\n",
    "    # 10.\tdepartment: The number of the department. See the next appendix for a department lookup table.\n",
    "    columns.append('department') # \n",
    "    # 11.\tquantity: The purchased quantity. Beware, some items such as flowers and bulk vegetables are priced per  # cent and then sold in very large quantities (like 1000 for a $10 bouquet.)   \n",
    "    columns.append('quantity')\n",
    "    # 12.\tScale: The reading on the scale. Note that the capital here is not a typo. This is one field that \n",
    "    columns.append('Scale') # weirdly has a capital first letter.    \n",
    "    # 13.\tcost: the per-unit cost of an item to the Wedge. This is not uniformly populated. \n",
    "    columns.append('cost') # \n",
    "    # 14.\tunitPrice:  the per-unit cost of an item to an owner. Negative for things like returns and discounts.\n",
    "    columns.append('unitPrice') # \n",
    "    # 15.\ttotal: price times quantity. The cost of the line item. Note that this can be negative because columns.append('total') # unitPrice can be negative.     \n",
    "    columns.append('total')\n",
    "    # 16.\tregPrice: The regular price of an item. May be different from unitPrice but unitPrice plus discount columns.append('regPrice') # should be regPrice.   \n",
    "    columns.append('regPrice')\n",
    "    # 17.\taltPrice\n",
    "    columns.append('altPrice') # \n",
    "    # 18.\ttax: an indicator of whether or not the item is taxable.   \n",
    "    columns.append('tax') # \n",
    "    # 19.\ttaxexempt: mostly zero.   \n",
    "    columns.append('taxexempt') # \n",
    "    # 20.\tfoodstamp: can the item be purchased with food stamps?   \n",
    "    columns.append('foodstamp') # \n",
    "    # 21.\twicable: can the item be purchased with WIC?   \n",
    "    columns.append('wicable') # \n",
    "    # 22.\tdiscount: a marker of any discounts.    \n",
    "    columns.append('discount') # \n",
    "    # 23.\tmemDiscount: the member discounts on items.   \n",
    "    columns.append('memDiscount') # \n",
    "    # 24.\tdiscountable: beats me.   \n",
    "    columns.append('discountable') # \n",
    "    # 25.\tdiscounttype: there’s probably information in here, but I haven’t decoded it.\n",
    "    columns.append('discounttype') # \n",
    "    # 26.\tvoided: I think it’s used if an item is a void or if an item was run up and subsequently voided.   \n",
    "    columns.append('voided') # \n",
    "    # 27.\tpercentDiscount: I don’t use it.   \n",
    "    columns.append('percentDiscount') # \n",
    "    # 28.\tItemQtty: I’m not sure what this is.   \n",
    "    columns.append('ItemQtty') # \n",
    "    # 29.\tvolDiscType: Ditto   \n",
    "    columns.append('volDiscType') # \n",
    "    # 30.\tvolume: Ditto\n",
    "    columns.append('volume') # \n",
    "    # 31.\tVolSpecial: Ditto   \n",
    "    columns.append('VolSpecial') # \n",
    "    # 32.\tmixMatch: Ditto   \n",
    "    columns.append('mixMatch') # \n",
    "    # 33.\tmatched: Ditto   \n",
    "    columns.append('matched') # \n",
    "    # 34.\tmemType: Mostly NULL or 1, but I’m not sure what it signifies. Maybe institutional memberships?   \n",
    "    columns.append('memType') # \n",
    "    # 35.\tstaff: indicative of staff transactions perhaps?   \n",
    "    columns.append('staff') # \n",
    "    # 36.\tnumflag: A complicated bitflag that encodes a bunch of other information. I’ll add the communication on columns.append('numflag') # this topic to an appendix below, but it’s not critical for our purposes.   \n",
    "    columns.append('numflag')\n",
    "    # 37.\tItemstatus: Don’t know   \n",
    "    columns.append('Itemstatus') # \n",
    "    # 38.\ttenderstatus: Ditto   \n",
    "    columns.append('tenderstatus') # \n",
    "    # 39.\tcharflag: Ditto   \n",
    "    columns.append('charflag') # \n",
    "    # 40.\tvarflag: Ditto   \n",
    "    columns.append('varflag') # \n",
    "    # 41.\tbatchHeaderID: Ditto   \n",
    "    columns.append('batchHeaderID') # \n",
    "    # 42.\tlocal: is the item local?   \n",
    "    columns.append('local') # \n",
    "    # 43.\torganic: is the item organic?   \n",
    "    columns.append('organic') # \n",
    "    # 44.\tdisplay: Don’t know.   \n",
    "    columns.append('display') # \n",
    "    # 45.\treceipt: Ditto   \n",
    "    columns.append('receipt') # \n",
    "    # 46.\tcard_no: This one is important. This is the masked owner number for the transaction. It is an integer. columns.append('card_no') # If the value is 3, then the transaction is for a non-owner. You’ll find some owners (like 11572) that have a huge number of transactions. These are likely other co-ops. If you are a member of, say, the Seward Co-op you can receive discounts at the Wedge. The cashier selects your co-op and the receipt is flagged as being from that co-op.    \n",
    "    columns.append('card_no')\n",
    "    # 47.\tstore: 1 for the main store and 512 for catering.   \n",
    "    columns.append('store') # \n",
    "    # 48.\tbranch: 0 for the main store and 3 for the Wedge Table, a grab-and-go bodega they opened in January columns.append('branch') # 2015.  \n",
    "    columns.append('branch')\n",
    "    # 49.\tmatch_id: don’t know   \n",
    "    columns.append('match_id') # \n",
    "    # 50.\ttrans_id: a counter that increments the line items of a receipt.\n",
    "    columns.append('trans_id') # \n",
    "    # print(columns)\n",
    "    return columns\n",
    "\n",
    "\n",
    "def dtype_columns():\n",
    "    # https://www.geeksforgeeks.org/add-column-names-to-dataframe-in-pandas/\n",
    "    columns = {}\n",
    "    # 1.\tdatetime: timestamp of the transaction-row creation   \n",
    "    columns.update({'datetime':'string'}) # \n",
    "    # 2.\tregister_no: register for transaction\n",
    "    columns.update({'register_no':'string'}) # \n",
    "    # 3.\temp_no: employee number for cashier   \n",
    "    columns.update({'emp_no':'string'}) # \n",
    "    # 4.\ttrans_no: transaction number. This number counts up by day and is only unique when combined with date, columns.update({'trans_no':'string'}) # register and employee.\n",
    "    columns.update({'trans_no':'string'})\n",
    "    # 5.\tUpc: Universal Product Code for the item. 0 for non-items.\n",
    "    columns.update({'Upc':'string'}) # \n",
    "    # 6.\tdescription: product description. Includes things like Tax, Tender type, etc.   \n",
    "    columns.update({'description':'string'}) # \n",
    "    # 7.\ttrans_type: One of five values (D, G, A, T, and I). These correspond to the following types of columns.update({'trans_type':'string'}) # transactions:\n",
    "    # •\tD: Departmental rings, when the cashier just selects a department for the item.\n",
    "    # •\tG: Green patch donations. This is the donation made for shoppers who bring their own bag. \n",
    "    # •\tA: Tax\n",
    "    # •\tT: Tender, the payment row.\n",
    "    # •\tI: Items, but also includes discounts. \n",
    "    columns.update({'trans_type':'string'})\n",
    "    # 8.\ttrans_subtype: There are a lot of these. Key ones include methods of payment (CK for Check, CA for columns.update({'trans_subtype':'string'}) # Cash, CP for coupon, EF for EBT Food Stamps , WC for WIC). These are often blank for other trans_type values.\n",
    "    columns.update({'trans_subtype':'string'})\n",
    "    # 9.\ttrans_status: An important field. The field trans_status tells us more about the types transactions. columns.update({'trans_status':'string'}) # Here are the possible values:\n",
    "    # •\tBlank: The typical value.\n",
    "    # •\tM: Member discounts.\n",
    "    # •\tV: Voids \n",
    "    # •\tC: Coupons\n",
    "    # •\t0: Honestly, I think these are supposed to be blanks but they changed from 0s at some point in February 2010. \n",
    "    # •\tR: Returns.\n",
    "    # •\tJ: Juice club cards\n",
    "    columns.update({'trans_status':'string'})\n",
    "    # 10.\tdepartment: The number of the department. See the next appendix for a department lookup table.\n",
    "    columns.update({'department':'string'}) # \n",
    "    # 11.\tquantity: The purchased quantity. Beware, some items such as flowers and bulk vegetables are priced per  # cent and then sold in very large quantities (like 1000 for a $10 bouquet.)   \n",
    "    columns.update({'quantity':'string'})\n",
    "    # 12.\tScale: The reading on the scale. Note that the capital here is not a typo. This is one field that \n",
    "    columns.update({'Scale':'string'}) # weirdly has a capital first letter.    \n",
    "    # 13.\tcost: the per-unit cost of an item to the Wedge. This is not uniformly populated. \n",
    "    columns.update({'cost':'string'}) # \n",
    "    # 14.\tunitPrice:  the per-unit cost of an item to an owner. Negative for things like returns and discounts.\n",
    "    columns.update({'unitPrice':'string'}) # \n",
    "    # 15.\ttotal: price times quantity. The cost of the line item. Note that this can be negative because columns.update({'total':'string'}) # unitPrice can be negative.     \n",
    "    columns.update({'total':'string'})\n",
    "    # 16.\tregPrice: The regular price of an item. May be different from unitPrice but unitPrice plus discount columns.update({'regPrice':'string'}) # should be regPrice.   \n",
    "    columns.update({'regPrice':'string'})\n",
    "    # 17.\taltPrice\n",
    "    columns.update({'altPrice':'string'}) # \n",
    "    # 18.\ttax: an indicator of whether or not the item is taxable.   \n",
    "    columns.update({'tax':'string'}) # \n",
    "    # 19.\ttaxexempt: mostly zero.   \n",
    "    columns.update({'taxexempt':'string'}) # \n",
    "    # 20.\tfoodstamp: can the item be purchased with food stamps?   \n",
    "    columns.update({'foodstamp':'string'}) # \n",
    "    # 21.\twicable: can the item be purchased with WIC?   \n",
    "    columns.update({'wicable':'string'}) # \n",
    "    # 22.\tdiscount: a marker of any discounts.    \n",
    "    columns.update({'discount':'string'}) # \n",
    "    # 23.\tmemDiscount: the member discounts on items.   \n",
    "    columns.update({'memDiscount':'string'}) # \n",
    "    # 24.\tdiscountable: beats me.   \n",
    "    columns.update({'discountable':'string'}) # \n",
    "    # 25.\tdiscounttype: there’s probably information in here, but I haven’t decoded it.\n",
    "    columns.update({'discounttype':'string'}) # \n",
    "    # 26.\tvoided: I think it’s used if an item is a void or if an item was run up and subsequently voided.   \n",
    "    columns.update({'voided':'string'}) # \n",
    "    # 27.\tpercentDiscount: I don’t use it.   \n",
    "    columns.update({'percentDiscount':'string'}) # \n",
    "    # 28.\tItemQtty: I’m not sure what this is.   \n",
    "    columns.update({'ItemQtty':'string'}) # \n",
    "    # 29.\tvolDiscType: Ditto   \n",
    "    columns.update({'volDiscType':'string'}) # \n",
    "    # 30.\tvolume: Ditto\n",
    "    columns.update({'volume':'string'}) # \n",
    "    # 31.\tVolSpecial: Ditto   \n",
    "    columns.update({'VolSpecial':'string'}) # \n",
    "    # 32.\tmixMatch: Ditto   \n",
    "    columns.update({'mixMatch':'string'}) # \n",
    "    # 33.\tmatched: Ditto   \n",
    "    columns.update({'matched':'string'}) # \n",
    "    # 34.\tmemType: Mostly NULL or 1, but I’m not sure what it signifies. Maybe institutional memberships?   \n",
    "    columns.update({'memType':'string'}) # \n",
    "    # 35.\tstaff: indicative of staff transactions perhaps?   \n",
    "    columns.update({'staff':'string'}) # \n",
    "    # 36.\tnumflag: A complicated bitflag that encodes a bunch of other information. I’ll add the communication on columns.update({'numflag':'string'}) # this topic to an appendix below, but it’s not critical for our purposes.   \n",
    "    columns.update({'numflag':'string'})\n",
    "    # 37.\tItemstatus: Don’t know   \n",
    "    columns.update({'Itemstatus':'string'}) # \n",
    "    # 38.\ttenderstatus: Ditto   \n",
    "    columns.update({'tenderstatus':'string'}) # \n",
    "    # 39.\tcharflag: Ditto   \n",
    "    columns.update({'charflag':'string'}) # \n",
    "    # 40.\tvarflag: Ditto   \n",
    "    columns.update({'varflag':'string'}) # \n",
    "    # 41.\tbatchHeaderID: Ditto   \n",
    "    columns.update({'batchHeaderID':'string'}) # \n",
    "    # 42.\tlocal: is the item local?   \n",
    "    columns.update({'local':'string'}) # \n",
    "    # 43.\torganic: is the item organic?   \n",
    "    columns.update({'organic':'string'}) # \n",
    "    # 44.\tdisplay: Don’t know.   \n",
    "    columns.update({'display':'string'}) # \n",
    "    # 45.\treceipt: Ditto   \n",
    "    columns.update({'receipt':'string'}) # \n",
    "    # 46.\tcard_no: This one is important. This is the masked owner number for the transaction. It is an integer. If the value is 3, then the transaction is for a non-owner. You’ll find some owners (like 11572) that have a huge number of transactions. These are likely other co-ops. If you are a member of, say, the Seward Co-op you can receive discounts at the Wedge. The cashier selects your co-op and the receipt is flagged as being from that co-op.    \n",
    "    columns.update({'card_no':'string'})\n",
    "    # 47.\tstore: 1 for the main store and 512 for catering.   \n",
    "    columns.update({'store':'string'}) # \n",
    "    # 48.\tbranch: 0 for the main store and 3 for the Wedge Table, a grab-and-go bodega they opened in January 2015.  \n",
    "    columns.update({'branch':'string'})\n",
    "    # 49.\tmatch_id: don’t know   \n",
    "    columns.update({'match_id':'string'}) # \n",
    "    # 50.\ttrans_id: a counter that increments the line items of a receipt.\n",
    "    columns.update({'trans_id':'string'}) # \n",
    "    \n",
    "    # print(columns)\n",
    "\n",
    "    return columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e933c8",
   "metadata": {},
   "source": [
    "## GBQ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5a19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine. \n",
    "# service_path = \"/Users/chandler/Dropbox/Teaching/\"\n",
    "# service_file = 'umt-msba-037daf11ee16.json' # change this to your authentication information  \n",
    "# gbq_proj_id = 'umt-msba' # change this to your project. \n",
    "# service_path = \"/media/psf/Home/Repos/\"\n",
    "service_path = \"/home/blackvwgolf95/\"\n",
    "service_file = 'bmkt670-fall2022-wedge-project-6ce4398b80e4.json' # change this to your authentication information  \n",
    "gbq_proj_id = 'bmkt670-fall2022-wedge-project' # change this to your project. \n",
    "dataset_id = 'wedgedataset'\n",
    "\n",
    "# And this should stay the same. \n",
    "private_key = service_path + service_file\n",
    "\n",
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "\n",
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)\n",
    "\n",
    "# for item in client.list_datasets() : \n",
    "#    print(item.full_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d2145",
   "metadata": {},
   "source": [
    "## Phase 1, Upload Clean Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6aad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all the files now...\n",
      "File Exists, skipping\n",
      "transArchive_201001_201003.zip\n",
      "File Exists, skipping\n",
      "transArchive_201004_201006.zip\n",
      "File Exists, skipping\n",
      "transArchive_201007_201009.zip\n",
      "File Exists, skipping\n",
      "transArchive_201010_201012.zip\n",
      "File Exists, skipping\n",
      "transArchive_201101_201103.zip\n",
      "File Exists, skipping\n",
      "transArchive_201104.zip\n",
      "File Exists, skipping\n",
      "transArchive_201105.zip\n",
      "File Exists, skipping\n",
      "transArchive_201106.zip\n",
      "File Exists, skipping\n",
      "transArchive_201107_201109.zip\n",
      "File Exists, skipping\n",
      "transArchive_201110_201112.zip\n",
      "File Exists, skipping\n",
      "transArchive_201201_201203.zip\n",
      "File Exists, skipping\n",
      "transArchive_201201_201203_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201204_201206.zip\n",
      "File Exists, skipping\n",
      "transArchive_201204_201206_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201207_201209.zip\n",
      "File Exists, skipping\n",
      "transArchive_201207_201209_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201210_201212.zip\n",
      "File Exists, skipping\n",
      "transArchive_201210_201212_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201301_201303.zip\n",
      "File Exists, skipping\n",
      "transArchive_201301_201303_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201304_201306.zip\n",
      "File Exists, skipping\n",
      "transArchive_201304_201306_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201307_201309.zip\n",
      "File Exists, skipping\n",
      "transArchive_201307_201309_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201310_201312.zip\n",
      "File Exists, skipping\n",
      "transArchive_201310_201312_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201401_201403.zip\n",
      "File Exists, skipping\n",
      "transArchive_201401_201403_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201404_201406.zip\n",
      "File Exists, skipping\n",
      "transArchive_201404_201406_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201407_201409.zip\n",
      "File Exists, skipping\n",
      "transArchive_201407_201409_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201410_201412.zip\n",
      "File Exists, skipping\n",
      "transArchive_201410_201412_inactive.zip\n",
      "File Exists, skipping\n",
      "transArchive_201501_201503.zip\n",
      "File Exists, skipping\n",
      "transArchive_201504_201506.zip\n",
      "File Exists, skipping\n",
      "transArchive_201507_201509.zip\n",
      "File Exists, skipping\n",
      "transArchive_201510.zip\n",
      "File Exists, skipping\n",
      "transArchive_201511.zip\n",
      "File Exists, skipping\n",
      "transArchive_201512.zip\n",
      "File Exists, skipping\n",
      "transArchive_201601.zip\n",
      "File Exists, skipping\n",
      "transArchive_201602.zip\n",
      "File Exists, skipping\n",
      "transArchive_201603.zip\n",
      "File Exists, skipping\n",
      "transArchive_201604.zip\n",
      "File Exists, skipping\n",
      "transArchive_201605.zip\n",
      "File Exists, skipping\n",
      "transArchive_201606.zip\n",
      "File Exists, skipping\n",
      "transArchive_201607.zip\n",
      "File Exists, skipping\n",
      "transArchive_201608.zip\n",
      "File Exists, skipping\n",
      "transArchive_201609.zip\n",
      "File Exists, skipping\n",
      "transArchive_201610.zip\n",
      "File Exists, skipping\n",
      "transArchive_201611.zip\n",
      "File Exists, skipping\n",
      "transArchive_201612.zip\n",
      "File Exists, skipping\n",
      "transArchive_201701.zip\n",
      "Done Extracting!\n",
      "Done building file list\n"
     ]
    }
   ],
   "source": [
    "# In this cell, do the following: \n",
    "\n",
    "# Master list of all data files\n",
    "zip_files = []\n",
    "\n",
    "with ZipFile( zip_file_name, 'r') as zf : \n",
    "    extract_zip(zf)\n",
    "    print('Done Extracting!')\n",
    "    \n",
    "\n",
    "print(\"Done building file list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1d17b",
   "metadata": {},
   "source": [
    "## Verify ZIP Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb0e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_file_contents(zip_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ed283",
   "metadata": {},
   "source": [
    "## Extract Inner Zips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e32e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists, skipping\n",
      "transArchive_201001_201003.csv\n",
      "File Exists, skipping\n",
      "transArchive_201004_201006.csv\n",
      "File Exists, skipping\n",
      "transArchive_201007_201009.csv\n",
      "File Exists, skipping\n",
      "transArchive_201010_201012.csv\n",
      "File Exists, skipping\n",
      "transArchive_201101_201103.csv\n",
      "File Exists, skipping\n",
      "transArchive_201104.csv\n",
      "File Exists, skipping\n",
      "transArchive_201105.csv\n",
      "File Exists, skipping\n",
      "transArchive_201106.csv\n",
      "File Exists, skipping\n",
      "transArchive_201107_201109.csv\n",
      "File Exists, skipping\n",
      "transArchive_201110_201112.csv\n",
      "File Exists, skipping\n",
      "transArchive_201201_201203.csv\n",
      "File Exists, skipping\n",
      "transArchive_201201_201203_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201204_201206.csv\n",
      "File Exists, skipping\n",
      "transArchive_201204_201206_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201207_201209.csv\n",
      "File Exists, skipping\n",
      "transArchive_201207_201209_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201210_201212.csv\n",
      "File Exists, skipping\n",
      "transArchive_201210_201212_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201301_201303.csv\n",
      "File Exists, skipping\n",
      "transArchive_201301_201303_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201304_201306.csv\n",
      "File Exists, skipping\n",
      "transArchive_201304_201306_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201307_201309.csv\n",
      "File Exists, skipping\n",
      "transArchive_201307_201309_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201310_201312.csv\n",
      "File Exists, skipping\n",
      "transArchive_201310_201312_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201401_201403.csv\n",
      "File Exists, skipping\n",
      "transArchive_201401_201403_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201404_201406.csv\n",
      "File Exists, skipping\n",
      "transArchive_201404_201406_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201407_201409.csv\n",
      "File Exists, skipping\n",
      "transArchive_201407_201409_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201410_201412.csv\n",
      "File Exists, skipping\n",
      "transArchive_201410_201412_inactive.csv\n",
      "File Exists, skipping\n",
      "transArchive_201501_201503.csv\n",
      "File Exists, skipping\n",
      "transArchive_201504_201506.csv\n",
      "File Exists, skipping\n",
      "transArchive_201507_201509.csv\n",
      "File Exists, skipping\n",
      "transArchive_201510.csv\n",
      "File Exists, skipping\n",
      "transArchive_201511.csv\n",
      "File Exists, skipping\n",
      "transArchive_201512.csv\n",
      "File Exists, skipping\n",
      "transArchive_201601.csv\n",
      "File Exists, skipping\n",
      "transArchive_201602.csv\n",
      "File Exists, skipping\n",
      "transArchive_201603.csv\n",
      "File Exists, skipping\n",
      "transArchive_201604.csv\n",
      "File Exists, skipping\n",
      "transArchive_201605.csv\n",
      "File Exists, skipping\n",
      "transArchive_201606.csv\n",
      "File Exists, skipping\n",
      "transArchive_201607.csv\n",
      "File Exists, skipping\n",
      "transArchive_201608.csv\n",
      "File Exists, skipping\n",
      "transArchive_201609.csv\n",
      "File Exists, skipping\n",
      "transArchive_201610.csv\n",
      "File Exists, skipping\n",
      "transArchive_201611.csv\n",
      "File Exists, skipping\n",
      "transArchive_201612.csv\n",
      "File Exists, skipping\n",
      "transArchive_201701.csv\n"
     ]
    }
   ],
   "source": [
    "data_files = []\n",
    "\n",
    "for inner_zip_file_name in zip_files :\n",
    "    # print(working_directory + inner_zip_file_name)\n",
    "    # Ignore folders\n",
    "    if not inner_zip_file_name.endswith( '.zip' ):\n",
    "        continue\n",
    "    with ZipFile( working_directory + inner_zip_file_name, 'r') as zf : \n",
    "        extract_single_zip(zf)\n",
    "#         extract_single_zips(zip_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac253e0",
   "metadata": {},
   "source": [
    "## Verify Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7da8097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_file_contents(data_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96663152",
   "metadata": {},
   "source": [
    "### Checking for and deleting previous tables\n",
    "\n",
    "We'll get all the tables in our Dram data set that match our pattern, then delete them. We do not want to accidentally delete the item lookup table that we put in this data set in class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde794db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at transArchive_201001_201003\n",
      "She swiped right, we have a MATCH! transArchive_201001_201003\n",
      "She blocked us, all hope is lost transArchive_201001_201003.\n",
      "Looking at transArchive_201004_201006\n",
      "She swiped right, we have a MATCH! transArchive_201004_201006\n",
      "She blocked us, all hope is lost transArchive_201004_201006.\n",
      "Looking at transArchive_201007_201009\n",
      "She swiped right, we have a MATCH! transArchive_201007_201009\n",
      "She blocked us, all hope is lost transArchive_201007_201009.\n",
      "Looking at transArchive_201010_201012\n",
      "She swiped right, we have a MATCH! transArchive_201010_201012\n",
      "She blocked us, all hope is lost transArchive_201010_201012.\n",
      "Looking at transArchive_201101_201103\n",
      "She swiped right, we have a MATCH! transArchive_201101_201103\n",
      "She blocked us, all hope is lost transArchive_201101_201103.\n",
      "Looking at transArchive_201104\n",
      "She swiped right, we have a MATCH! transArchive_201104\n",
      "She blocked us, all hope is lost transArchive_201104.\n",
      "Looking at transArchive_201105\n",
      "She swiped right, we have a MATCH! transArchive_201105\n",
      "She blocked us, all hope is lost transArchive_201105.\n",
      "Looking at transArchive_201106\n",
      "She swiped right, we have a MATCH! transArchive_201106\n",
      "She blocked us, all hope is lost transArchive_201106.\n",
      "Looking at transArchive_201107_201109\n",
      "She swiped right, we have a MATCH! transArchive_201107_201109\n",
      "She blocked us, all hope is lost transArchive_201107_201109.\n",
      "Looking at transArchive_201110_201112\n",
      "She swiped right, we have a MATCH! transArchive_201110_201112\n",
      "She blocked us, all hope is lost transArchive_201110_201112.\n",
      "Looking at transArchive_201201_201203\n",
      "She swiped right, we have a MATCH! transArchive_201201_201203\n",
      "She blocked us, all hope is lost transArchive_201201_201203.\n",
      "Looking at transArchive_201201_201203_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201201_201203_inactive\n",
      "She blocked us, all hope is lost transArchive_201201_201203_inactive.\n",
      "Looking at transArchive_201204_201206\n",
      "She swiped right, we have a MATCH! transArchive_201204_201206\n",
      "She blocked us, all hope is lost transArchive_201204_201206.\n",
      "Looking at transArchive_201204_201206_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201204_201206_inactive\n",
      "She blocked us, all hope is lost transArchive_201204_201206_inactive.\n",
      "Looking at transArchive_201207_201209\n",
      "She swiped right, we have a MATCH! transArchive_201207_201209\n",
      "She blocked us, all hope is lost transArchive_201207_201209.\n",
      "Looking at transArchive_201207_201209_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201207_201209_inactive\n",
      "She blocked us, all hope is lost transArchive_201207_201209_inactive.\n",
      "Looking at transArchive_201210_201212\n",
      "She swiped right, we have a MATCH! transArchive_201210_201212\n",
      "She blocked us, all hope is lost transArchive_201210_201212.\n",
      "Looking at transArchive_201210_201212_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201210_201212_inactive\n",
      "She blocked us, all hope is lost transArchive_201210_201212_inactive.\n",
      "Looking at transArchive_201301_201303\n",
      "She swiped right, we have a MATCH! transArchive_201301_201303\n",
      "She blocked us, all hope is lost transArchive_201301_201303.\n",
      "Looking at transArchive_201301_201303_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201301_201303_inactive\n",
      "She blocked us, all hope is lost transArchive_201301_201303_inactive.\n",
      "Looking at transArchive_201304_201306\n",
      "She swiped right, we have a MATCH! transArchive_201304_201306\n",
      "She blocked us, all hope is lost transArchive_201304_201306.\n",
      "Looking at transArchive_201304_201306_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201304_201306_inactive\n",
      "She blocked us, all hope is lost transArchive_201304_201306_inactive.\n",
      "Looking at transArchive_201307_201309\n",
      "She swiped right, we have a MATCH! transArchive_201307_201309\n",
      "She blocked us, all hope is lost transArchive_201307_201309.\n",
      "Looking at transArchive_201307_201309_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201307_201309_inactive\n",
      "She blocked us, all hope is lost transArchive_201307_201309_inactive.\n",
      "Looking at transArchive_201310_201312\n",
      "She swiped right, we have a MATCH! transArchive_201310_201312\n",
      "She blocked us, all hope is lost transArchive_201310_201312.\n",
      "Looking at transArchive_201310_201312_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201310_201312_inactive\n",
      "She blocked us, all hope is lost transArchive_201310_201312_inactive.\n",
      "Looking at transArchive_201401_201403\n",
      "She swiped right, we have a MATCH! transArchive_201401_201403\n",
      "She blocked us, all hope is lost transArchive_201401_201403.\n",
      "Looking at transArchive_201401_201403_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201401_201403_inactive\n",
      "She blocked us, all hope is lost transArchive_201401_201403_inactive.\n",
      "Looking at transArchive_201404_201406\n",
      "She swiped right, we have a MATCH! transArchive_201404_201406\n",
      "She blocked us, all hope is lost transArchive_201404_201406.\n",
      "Looking at transArchive_201404_201406_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201404_201406_inactive\n",
      "She blocked us, all hope is lost transArchive_201404_201406_inactive.\n",
      "Looking at transArchive_201407_201409\n",
      "She swiped right, we have a MATCH! transArchive_201407_201409\n",
      "She blocked us, all hope is lost transArchive_201407_201409.\n",
      "Looking at transArchive_201407_201409_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201407_201409_inactive\n",
      "She blocked us, all hope is lost transArchive_201407_201409_inactive.\n",
      "Looking at transArchive_201410_201412\n",
      "She swiped right, we have a MATCH! transArchive_201410_201412\n",
      "She blocked us, all hope is lost transArchive_201410_201412.\n",
      "Looking at transArchive_201410_201412_inactive\n",
      "She swiped right, we have a MATCH! transArchive_201410_201412_inactive\n",
      "She blocked us, all hope is lost transArchive_201410_201412_inactive.\n",
      "Looking at transArchive_201501_201503\n",
      "She swiped right, we have a MATCH! transArchive_201501_201503\n",
      "She blocked us, all hope is lost transArchive_201501_201503.\n",
      "Looking at transArchive_201504_201506\n",
      "She swiped right, we have a MATCH! transArchive_201504_201506\n",
      "She blocked us, all hope is lost transArchive_201504_201506.\n",
      "Looking at transArchive_201507_201509\n",
      "She swiped right, we have a MATCH! transArchive_201507_201509\n",
      "She blocked us, all hope is lost transArchive_201507_201509.\n",
      "Looking at transArchive_201510\n",
      "She swiped right, we have a MATCH! transArchive_201510\n",
      "She blocked us, all hope is lost transArchive_201510.\n",
      "Looking at transArchive_201511\n",
      "She swiped right, we have a MATCH! transArchive_201511\n",
      "She blocked us, all hope is lost transArchive_201511.\n",
      "Looking at transArchive_201512\n",
      "She swiped right, we have a MATCH! transArchive_201512\n",
      "She blocked us, all hope is lost transArchive_201512.\n",
      "Looking at transArchive_201601\n",
      "She swiped right, we have a MATCH! transArchive_201601\n",
      "She blocked us, all hope is lost transArchive_201601.\n",
      "Looking at transArchive_201602\n",
      "She swiped right, we have a MATCH! transArchive_201602\n",
      "She blocked us, all hope is lost transArchive_201602.\n",
      "Looking at transArchive_201603\n",
      "She swiped right, we have a MATCH! transArchive_201603\n",
      "She blocked us, all hope is lost transArchive_201603.\n",
      "Looking at transArchive_201604\n",
      "She swiped right, we have a MATCH! transArchive_201604\n",
      "She blocked us, all hope is lost transArchive_201604.\n",
      "Looking at transArchive_201605\n",
      "She swiped right, we have a MATCH! transArchive_201605\n",
      "She blocked us, all hope is lost transArchive_201605.\n",
      "Looking at transArchive_201606\n",
      "She swiped right, we have a MATCH! transArchive_201606\n",
      "She blocked us, all hope is lost transArchive_201606.\n",
      "Looking at transArchive_201607\n",
      "She swiped right, we have a MATCH! transArchive_201607\n",
      "She blocked us, all hope is lost transArchive_201607.\n",
      "Looking at transArchive_201608\n",
      "She swiped right, we have a MATCH! transArchive_201608\n",
      "She blocked us, all hope is lost transArchive_201608.\n",
      "Looking at transArchive_201609\n",
      "She swiped right, we have a MATCH! transArchive_201609\n",
      "She blocked us, all hope is lost transArchive_201609.\n",
      "Looking at transArchive_201610\n",
      "She swiped right, we have a MATCH! transArchive_201610\n",
      "She blocked us, all hope is lost transArchive_201610.\n",
      "Looking at transArchive_201611\n",
      "She swiped right, we have a MATCH! transArchive_201611\n",
      "She blocked us, all hope is lost transArchive_201611.\n",
      "Looking at transArchive_201612\n",
      "She swiped right, we have a MATCH! transArchive_201612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She blocked us, all hope is lost transArchive_201612.\n",
      "Looking at transArchive_201701\n",
      "She swiped right, we have a MATCH! transArchive_201701\n",
      "She blocked us, all hope is lost transArchive_201701.\n"
     ]
    }
   ],
   "source": [
    "# create a regex that matches our table pattern\n",
    "# ymd_pattern = re.compile(r\"^dram_items_[1-2][9,0][1-2][9,0,1,2][01][0-9][01][0-9]$\") \n",
    "\n",
    "transArchive_pattern = re.compile(r\"^transArchive_*\") \n",
    "\n",
    "tables = client.list_tables(dataset_id)  \n",
    "\n",
    "for table in tables:\n",
    "    \n",
    "    print(f'Looking at {table.table_id}')\n",
    "\n",
    "    # Test to see if table.table_id matches the pattern\n",
    "    # if so, delete it\n",
    "    if transArchive_pattern.match(table.table_id):\n",
    "        # print(table.table_id)\n",
    "        print(f'She swiped right, we have a MATCH! {table.table_id}')\n",
    "        # table_id = \".\".join([gbq_proj_id,dataset_id,table.table_id])\n",
    "        # Disabling to prevent accidently running\n",
    "        client.delete_table(table, not_found_ok=True)\n",
    "        print(f\"She blocked us, all hope is lost {table.table_id}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad37f65",
   "metadata": {},
   "source": [
    "## Uploading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2896bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def upload_file(file_name):\n",
    "#     # https://stackoverflow.com/a/27232309\n",
    "#     transactions = pd.read_csv(working_directory+file_name, \n",
    "#                                header=None, \n",
    "#                                names=data_columns(), \n",
    "#                                dtype=dtype_columns()\n",
    "#                               ) # \n",
    "\n",
    "#     # Construct table name from index\n",
    "#     table_name = \"wedge_\"+file_name.replace(\".\",\"-\").replace(\"/\",\"-\")\n",
    "#     # print(type(name))\n",
    "\n",
    "#     # 3. For each month in the file, subset the data to that month and \n",
    "#     #    upload the data to a table called `dram_items_YYYYMM01`. \n",
    "#     table_id = \".\".join([gbq_proj_id,dataset_id,table_name])\n",
    "    \n",
    "#     # print(table_id)\n",
    "    \n",
    "#     job = client.load_table_from_dataframe(\n",
    "#         transactions, table_id, job_config=job_config\n",
    "#     ) # \n",
    "\n",
    "#     # Wait for the load job to complete. (I omit this step)\n",
    "#     print(job.result())\n",
    "\n",
    "def upload_file(file_name):\n",
    "    # print(file_name)\n",
    "    print(\"Starting Processing File: \" + file_name + \"\\n\")\n",
    "    delimiter = get_delimiter(file_name)\n",
    "    \n",
    "    header = get_header(file_name)\n",
    "    \n",
    "    # https://stackoverflow.com/a/27232309\n",
    "    transactions = pd.read_csv(working_directory+file_name, \n",
    "                               header=header, \n",
    "                               names=data_columns(), \n",
    "                               dtype=dtype_columns(),\n",
    "                               delimiter=delimiter,\n",
    "                               na_filter=False,\n",
    "                               na_values=['nan', 'NaN', 'null', 'NULL', '\\\\N'],\n",
    "                               escapechar=\"\\\\\"\n",
    "                              ) #\n",
    "    \n",
    "    transactions['datetime'] = transactions['datetime'].astype('datetime64[ns]')\n",
    "    \n",
    "    transactions['register_no'] = transactions['register_no'].astype('int')\n",
    "    transactions['emp_no'] = transactions['register_no'].astype('int')\n",
    "    transactions['trans_no'] = transactions['register_no'].astype('int')\n",
    "    transactions['department'] = transactions['department'].astype('int')\n",
    "    transactions['quantity'] = transactions['quantity'].astype('float')\n",
    "    transactions['Scale'] = transactions['Scale'].astype('float')\n",
    "    transactions['cost'] = transactions['cost'].astype('float')\n",
    "    transactions['unitPrice'] = transactions['unitPrice'].astype('float')\n",
    "    transactions['total'] = transactions['total'].astype('float')\n",
    "    transactions['regPrice'] = transactions['regPrice'].astype('float')\n",
    "    # transactions['altPrice'] = transactions['altPrice'].astype('float')\n",
    "    transactions['tax'] = transactions['tax'].astype('int')\n",
    "    transactions['foodstamp'] = transactions['foodstamp'].astype('int')\n",
    "    \n",
    "    transactions['trans_id'] = transactions['trans_id'].astype('int')\n",
    "  \n",
    "\n",
    "    # Construct table name from index\n",
    "    table_name = file_name.replace(\".csv\",\"\").replace(\"_small\",\"\").replace(\".\",\"-\").replace(\"/\",\"-\")\n",
    "    # print(type(name))\n",
    "    print(\"Uploading Table: \" + table_name + \"\\n\")\n",
    "    table_id = \".\".join([gbq_proj_id,dataset_id,table_name])\n",
    "    pandas_gbq.to_gbq(transactions, table_id, project_id=gbq_proj_id,if_exists=\"replace\") # let's discuss this last bit\n",
    "    \n",
    "    print(\"Finished Processing File: \" + file_name + \"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23126fbd",
   "metadata": {},
   "source": [
    "## Multi Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1312f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/68806012\n",
    "\n",
    "# Since string columns use the \"object\" dtype, pass in a (partial) schema\n",
    "# to ensure the correct BigQuery data type.\n",
    "# job_config = bigquery.LoadJobConfig(schema=[\n",
    "#     dtype_columns(),\n",
    "# ])\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\",)\n",
    "\n",
    "x = 0\n",
    "\n",
    "# 1. Read in the items files one at a time.\n",
    "for file_name in data_files :\n",
    "    print(\"Processing File: \" + file_name)\n",
    "    # upload_file(file_name)\n",
    "    # threading.Thread(target=upload_file, args=(file_name,)).start()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    x = x + 1\n",
    "    if x > 5:\n",
    "        break\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bf9e6e",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0bbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pool_size = 50  # your \"parallelness\"\n",
    "\n",
    "pool = Pool(pool_size)\n",
    "\n",
    "for file_name in data_files :\n",
    "    #pool.apply_async(upload_file, (file_name,))\n",
    "    pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79576f5f",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641e47be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Processing File: transArchive_201001_201003.csv\n",
      "\n",
      "Starting Processing File: transArchive_201004_201006.csv\n",
      "\n",
      "Starting Processing File: transArchive_201007_201009.csv\n",
      "\n",
      "Starting Processing File: transArchive_201010_201012.csv\n",
      "\n",
      "Starting Processing File: transArchive_201101_201103.csv\n",
      "\n",
      "Starting Processing File: transArchive_201104.csv\n",
      "\n",
      "Starting Processing File: transArchive_201105.csv\n",
      "\n",
      "Starting Processing File: transArchive_201106.csv\n",
      "\n",
      "Starting Processing File: transArchive_201107_201109.csv\n",
      "\n",
      "Starting Processing File: transArchive_201110_201112.csv\n",
      "\n",
      "Starting Processing File: transArchive_201201_201203.csv\n",
      "\n",
      "Starting Processing File: transArchive_201201_201203_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201204_201206.csv\n",
      "\n",
      "Starting Processing File: transArchive_201204_201206_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201207_201209.csv\n",
      "\n",
      "Starting Processing File: transArchive_201207_201209_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201210_201212.csv\n",
      "\n",
      "Starting Processing File: transArchive_201210_201212_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201301_201303.csv\n",
      "\n",
      "Starting Processing File: transArchive_201301_201303_inactive.csv\n",
      "\n",
      "Uploading Table: transArchive_201201_201203_inactive\n",
      "\n",
      "Starting Processing File: transArchive_201304_201306.csv\n",
      "\n",
      "Starting Processing File: transArchive_201304_201306_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201307_201309.csv\n",
      "\n",
      "Uploading Table: transArchive_201204_201206_inactive\n",
      "\n",
      "Starting Processing File: transArchive_201307_201309_inactive.csv\n",
      "\n",
      "Uploading Table: transArchive_201207_201209_inactive\n",
      "\n",
      "Starting Processing File: transArchive_201310_201312.csv\n",
      "\n",
      "Uploading Table: transArchive_201210_201212_inactive\n",
      "\n",
      "Starting Processing File: transArchive_201310_201312_inactive.csv\n",
      "\n",
      "Uploading Table: transArchive_201301_201303_inactive\n",
      "\n",
      "Starting Processing File: transArchive_201401_201403.csv\n",
      "\n",
      "Starting Processing File: transArchive_201401_201403_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201404_201406.csv\n",
      "\n",
      "Uploading Table: transArchive_201304_201306_inactive\n",
      "\n",
      "Uploading Table: transArchive_201307_201309_inactive\n",
      "\n",
      "Starting Processing File: transArchive_201404_201406_inactive.csv\n",
      "\n",
      "Uploading Table: transArchive_201310_201312_inactive\n",
      "\n",
      "Uploading Table: transArchive_201401_201403_inactive\n",
      "\n",
      "Starting Processing File: transArchive_201407_201409.csv\n",
      "\n",
      "Starting Processing File: transArchive_201407_201409_inactive.csv\n",
      "\n",
      "Uploading Table: transArchive_201404_201406_inactive\n",
      "\n",
      "Starting Processing File: transArchive_201410_201412.csv\n",
      "\n",
      "Uploading Table: transArchive_201407_201409_inactive\n",
      "\n",
      "Starting Processing File: transArchive_201410_201412_inactive.csv\n",
      "\n",
      "Uploading Table: transArchive_201410_201412_inactive\n",
      "\n",
      "Starting Processing File: transArchive_201501_201503.csv\n",
      "\n",
      "Starting Processing File: transArchive_201504_201506.csv\n",
      "\n",
      "Starting Processing File: transArchive_201507_201509.csv\n",
      "\n",
      "Starting Processing File: transArchive_201510.csv\n",
      "\n",
      "Uploading Table: transArchive_201104\n",
      "\n",
      "Starting Processing File: transArchive_201511.csv\n",
      "\n",
      "Finished Processing File: transArchive_201210_201212_inactive.csv\n",
      "\n",
      "Uploading Table: transArchive_201105\n",
      "\n",
      "Starting Processing File: transArchive_201512.csv\n",
      "\n",
      "Uploading Table: transArchive_201106\n",
      "\n",
      "Starting Processing File: transArchive_201601.csv\n",
      "\n",
      "Starting Processing File: transArchive_201602.csv\n",
      "\n",
      "Finished Processing File: transArchive_201410_201412_inactive.csv\n",
      "\n",
      "Finished Processing File: transArchive_201201_201203_inactive.csv\n",
      "\n",
      "Finished Processing File: transArchive_201401_201403_inactive.csv\n",
      "Starting Processing File: transArchive_201603.csv\n",
      "\n",
      "\n",
      "Finished Processing File: transArchive_201407_201409_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201604.csv\n",
      "\n",
      "Finished Processing File: transArchive_201310_201312_inactive.csv\n",
      "\n",
      "Finished Processing File: transArchive_201304_201306_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201605.csv\n",
      "\n",
      "Starting Processing File: transArchive_201606.csv\n",
      "\n",
      "Finished Processing File: transArchive_201404_201406_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201607.csv\n",
      "\n",
      "Starting Processing File: transArchive_201608.csv\n",
      "\n",
      "Finished Processing File: transArchive_201207_201209_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201609.csv\n",
      "Finished Processing File: transArchive_201301_201303_inactive.csv\n",
      "\n",
      "\n",
      "Starting Processing File: transArchive_201610.csv\n",
      "\n",
      "Finished Processing File: transArchive_201307_201309_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201611.csv\n",
      "\n",
      "Finished Processing File: transArchive_201204_201206_inactive.csv\n",
      "\n",
      "Starting Processing File: transArchive_201612.csv\n",
      "\n",
      "Starting Processing File: transArchive_201701.csv\n",
      "\n",
      "Uploading Table: transArchive_201510\n",
      "\n",
      "Uploading Table: transArchive_201511\n",
      "\n",
      "Uploading Table: transArchive_201512\n",
      "\n",
      "Uploading Table: transArchive_201602\n",
      "\n",
      "Uploading Table: transArchive_201601\n",
      "\n",
      "Uploading Table: transArchive_201606\n",
      "\n",
      "Uploading Table: transArchive_201604\n",
      "\n",
      "Uploading Table: transArchive_201603\n",
      "Uploading Table: transArchive_201608\n",
      "\n",
      "\n",
      "Uploading Table: transArchive_201607\n",
      "\n",
      "Uploading Table: transArchive_201605\n",
      "\n",
      "Uploading Table: transArchive_201609\n",
      "\n",
      "Uploading Table: transArchive_201610\n",
      "\n",
      "Uploading Table: transArchive_201611\n",
      "\n",
      "Uploading Table: transArchive_201612\n",
      "\n",
      "Uploading Table: transArchive_201701\n",
      "\n",
      "Finished Processing File: transArchive_201105.csv\n",
      "\n",
      "Finished Processing File: transArchive_201106.csv\n",
      "\n",
      "Finished Processing File: transArchive_201104.csv\n",
      "\n",
      "Uploading Table: transArchive_201001_201003\n",
      "\n",
      "Uploading Table: transArchive_201007_201009\n",
      "\n",
      "Uploading Table: transArchive_201004_201006\n",
      "\n",
      "Uploading Table: transArchive_201101_201103\n",
      "\n",
      "Uploading Table: transArchive_201010_201012\n",
      "\n",
      "Finished Processing File: transArchive_201512.csv\n",
      "\n",
      "Finished Processing File: transArchive_201602.csv\n",
      "\n",
      "Finished Processing File: transArchive_201511.csv\n",
      "\n",
      "Uploading Table: transArchive_201107_201109\n",
      "\n",
      "Finished Processing File: transArchive_201510.csv\n",
      "\n",
      "Uploading Table: transArchive_201301_201303\n",
      "\n",
      "Uploading Table: transArchive_201201_201203\n",
      "\n",
      "Finished Processing File: transArchive_201609.csv\n",
      "\n",
      "Uploading Table: transArchive_201307_201309\n",
      "\n",
      "Finished Processing File: transArchive_201601.csv\n",
      "\n",
      "Finished Processing File: transArchive_201605.csv\n",
      "\n",
      "Uploading Table: transArchive_201210_201212\n",
      "\n",
      "Finished Processing File: transArchive_201603.csv\n",
      "\n",
      "Uploading Table: transArchive_201110_201112\n",
      "\n",
      "Uploading Table: transArchive_201204_201206\n",
      "\n",
      "Uploading Table: transArchive_201401_201403\n",
      "\n",
      "Uploading Table: transArchive_201310_201312\n",
      "\n",
      "Uploading Table: transArchive_201207_201209\n",
      "\n",
      "Uploading Table: transArchive_201304_201306\n",
      "Uploading Table: transArchive_201410_201412\n",
      "\n",
      "\n",
      "Finished Processing File: transArchive_201608.csv\n",
      "\n",
      "Finished Processing File: transArchive_201606.csv\n",
      "\n",
      "Finished Processing File: transArchive_201607.csv\n",
      "\n",
      "Uploading Table: transArchive_201407_201409\n",
      "\n",
      "Uploading Table: transArchive_201501_201503\n",
      "\n",
      "Uploading Table: transArchive_201404_201406\n",
      "\n",
      "Uploading Table: transArchive_201507_201509\n",
      "\n",
      "Finished Processing File: transArchive_201610.csv\n",
      "\n",
      "Uploading Table: transArchive_201504_201506\n",
      "\n",
      "Finished Processing File: transArchive_201604.csv\n",
      "\n",
      "Finished Processing File: transArchive_201611.csv\n",
      "\n",
      "Finished Processing File: transArchive_201612.csv\n",
      "\n",
      "Finished Processing File: transArchive_201701.csv\n",
      "\n",
      "Finished Processing File: transArchive_201201_201203.csv\n",
      "\n",
      "Finished Processing File: transArchive_201101_201103.csv\n",
      "\n",
      "Finished Processing File: transArchive_201004_201006.csv\n",
      "\n",
      "Finished Processing File: transArchive_201407_201409.csv\n",
      "\n",
      "Finished Processing File: transArchive_201401_201403.csv\n",
      "\n",
      "Finished Processing File: transArchive_201207_201209.csv\n",
      "\n",
      "Finished Processing File: transArchive_201107_201109.csv\n",
      "\n",
      "Finished Processing File: transArchive_201210_201212.csv\n",
      "\n",
      "Finished Processing File: transArchive_201001_201003.csv\n",
      "\n",
      "Finished Processing File: transArchive_201304_201306.csv\n",
      "\n",
      "Finished Processing File: transArchive_201010_201012.csv\n",
      "\n",
      "Finished Processing File: transArchive_201301_201303.csv\n",
      "\n",
      "Finished Processing File: transArchive_201410_201412.csv\n",
      "\n",
      "Finished Processing File: transArchive_201204_201206.csv\n",
      "\n",
      "Finished Processing File: transArchive_201501_201503.csv\n",
      "\n",
      "Finished Processing File: transArchive_201504_201506.csv\n",
      "\n",
      "Finished Processing File: transArchive_201507_201509.csv\n",
      "\n",
      "Finished Processing File: transArchive_201007_201009.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Processing File: transArchive_201307_201309.csv\n",
      "\n",
      "Finished Processing File: transArchive_201110_201112.csv\n",
      "\n",
      "Finished Processing File: transArchive_201310_201312.csv\n",
      "\n",
      "Finished Processing File: transArchive_201404_201406.csv\n",
      "\n",
      "Done!\n",
      "--- 218.95518279075623 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "jobs = []\n",
    "for file_name in data_files :\n",
    "    p = multiprocessing.Process(target=upload_file, args=(file_name,))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "    time.sleep(.5)\n",
    "    \n",
    "for p in jobs:\n",
    "    p.join()\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a81304",
   "metadata": {},
   "source": [
    "# Cleanup ALL Local Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://linuxize.com/post/python-delete-files-and-directories/\n",
    "try:\n",
    "    # shutil.rmtree(working_directory)\n",
    "    print('Done Cleanup')\n",
    "    print(\"Completed Exit Code 0\")\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (working_directory, e.strerror))\n",
    "    print(\"Completed Exit Code -1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
