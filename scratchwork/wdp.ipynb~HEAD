{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0533db",
   "metadata": {},
   "source": [
    "# The Wedge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2df271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import datetime \n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install janitor\n",
    "# !{sys.executable} -m pip install pyjanitor\n",
    "# !{sys.executable} -m pip install pandas_gbq\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import janitor\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c3586",
   "metadata": {},
   "source": [
    "## Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804044b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small File Sample\n",
    "zip_file_name = \"WedgeZipOfZips_Small.zip\"\n",
    "\n",
    "## Full data Set\n",
    "# zip_file_name = \"WedgeZipOfZips.zip\"\n",
    "\n",
    "# Clean data Set\n",
    "# zip_file_name = \"WedgeFiles_Clean.zip\"\n",
    "\n",
    "# Working Directory\n",
    "working_directory = \"/home/blackvwgolf95/BMKT670.V60-72020-Fall2022-Wedge-Project/eggs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184b104",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ea211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_zip_contents(zipped_files):\n",
    "    for file_name in zipped_files :\n",
    "        # Ignore __MACOSX hidden files\n",
    "        if(file_name.startswith( '__' )):\n",
    "            continue\n",
    "        # Ignore folders\n",
    "        if(file_name.endswith( '/' )):\n",
    "            continue\n",
    "\n",
    "        print(\"File: \", file_name,\" Size:\", os.path.getsize(working_directory+file_name))\n",
    "\n",
    "def display_file_contents(files):\n",
    "    for file_name in files :\n",
    "        # Ignore __MACOSX hidden files\n",
    "        if(file_name.startswith( '__' )):\n",
    "            continue\n",
    "        # Ignore folders\n",
    "        if(file_name.endswith( '/' )):\n",
    "            continue\n",
    "\n",
    "        print(\"File: \", file_name,\" Size:\", os.path.getsize(working_directory+file_name))\n",
    "\n",
    "def upload_data(data):\n",
    "    # https://stackoverflow.com/a/24083253\n",
    "    grouped = data.groupby(pd.Grouper(freq='M'))\n",
    "    for name, group in grouped:\n",
    "\n",
    "        # Construct table name from index\n",
    "        # table_name = \"dram_items_\"+reformat_date(name.strftime('%Y-%m-%d'))\n",
    "\n",
    "        # 3. For each month in the file, subset the data to that month and \n",
    "        #    upload the data to a table called `dram_items_YYYYMM01`. \n",
    "        # table_id = \".\".join([gbq_proj_id,dataset_id,table_name])\n",
    "        # print(table_id)\n",
    "        # pandas_gbq.to_gbq(item_lu, table_id, project_id=gbq_proj_id,if_exists=\"replace\") # let's discuss this last bit\n",
    "        print(\"Data Uploaded!\")\n",
    "        \n",
    "def cleanup_data(data): \n",
    "    # Clean the names with the janitor package.\n",
    "    data = janitor.clean_names(data)\n",
    "\n",
    "#     for column in ( 'gross_sales', 'discounts', 'net_sales', 'tax' ):\n",
    "#         # Convert the fields that have dollar signs (such as `gross_sales`) into numeric data. Watch out for dollar signs and commas.\n",
    "#         data[column] = ( data[column]\n",
    "#                            .str.replace(\"$\", '', regex=False)\n",
    "#                            .str.replace(\",\", '', regex=False)\n",
    "#                            .astype(float) )\n",
    "\n",
    "    # Change the type of the column `modifiers_applied` to string.\n",
    "#     data['modifiers_applied'] = data['modifiers_applied'].astype(str)\n",
    "\n",
    "    # Replace the `sku` column with a column of empty strings. \n",
    "#     data['sku'] = ''\n",
    "\n",
    "    # print( item_lu.head() )\n",
    "#     data.index = pd.to_datetime(data['date']) # ,format='%y-%m-%d'  \n",
    "    return data\n",
    "        \n",
    "def process_file(file_name):\n",
    "    # print(file_name)\n",
    "    data = pd.read_csv(working_directory+file_name, low_memory=False)\n",
    "  \n",
    "    # 2. Do the same cleaning we did in Part 1 (clean names, \n",
    "    #    make sku an empty string, fix dollars, make modifiers_applied a string)\n",
    "    data = cleanup_data(data)\n",
    "    upload_data(data)\n",
    "\n",
    "\n",
    "def data_columns():\n",
    "    # https://www.geeksforgeeks.org/add-column-names-to-dataframe-in-pandas/\n",
    "    columns = []\n",
    "    # 1.\tdatetime: timestamp of the transaction-row creation   \n",
    "    columns.append('datetime') # \n",
    "    # 2.\tregister_no: register for transaction\n",
    "    columns.append('register_no') # \n",
    "    # 3.\temp_no: employee number for cashier   \n",
    "    columns.append('emp_no') # \n",
    "    # 4.\ttrans_no: transaction number. This number counts up by day and is only unique when combined with date, columns.append('trans_no') # register and employee.\n",
    "    columns.append('trans_no')\n",
    "    # 5.\tUpc: Universal Product Code for the item. 0 for non-items.\n",
    "    columns.append('Upc') # \n",
    "    # 6.\tdescription: product description. Includes things like Tax, Tender type, etc.   \n",
    "    columns.append('description') # \n",
    "    # 7.\ttrans_type: One of five values (D, G, A, T, and I). These correspond to the following types of columns.append('trans_type') # transactions:\n",
    "    # •\tD: Departmental rings, when the cashier just selects a department for the item.\n",
    "    # •\tG: Green patch donations. This is the donation made for shoppers who bring their own bag. \n",
    "    # •\tA: Tax\n",
    "    # •\tT: Tender, the payment row.\n",
    "    # •\tI: Items, but also includes discounts. \n",
    "    columns.append('trans_type')\n",
    "    # 8.\ttrans_subtype: There are a lot of these. Key ones include methods of payment (CK for Check, CA for columns.append('trans_subtype') # Cash, CP for coupon, EF for EBT Food Stamps , WC for WIC). These are often blank for other trans_type values.\n",
    "    columns.append('trans_subtype')\n",
    "    # 9.\ttrans_status: An important field. The field trans_status tells us more about the types transactions. columns.append('trans_status') # Here are the possible values:\n",
    "    # •\tBlank: The typical value.\n",
    "    # •\tM: Member discounts.\n",
    "    # •\tV: Voids \n",
    "    # •\tC: Coupons\n",
    "    # •\t0: Honestly, I think these are supposed to be blanks but they changed from 0s at some point in February 2010. \n",
    "    # •\tR: Returns.\n",
    "    # •\tJ: Juice club cards\n",
    "    columns.append('trans_status')\n",
    "    # 10.\tdepartment: The number of the department. See the next appendix for a department lookup table.\n",
    "    columns.append('department') # \n",
    "    # 11.\tquantity: The purchased quantity. Beware, some items such as flowers and bulk vegetables are priced per  # cent and then sold in very large quantities (like 1000 for a $10 bouquet.)   \n",
    "    columns.append('quantity')\n",
    "    # 12.\tScale: The reading on the scale. Note that the capital here is not a typo. This is one field that \n",
    "    columns.append('Scale') # weirdly has a capital first letter.    \n",
    "    # 13.\tcost: the per-unit cost of an item to the Wedge. This is not uniformly populated. \n",
    "    columns.append('cost') # \n",
    "    # 14.\tunitPrice:  the per-unit cost of an item to an owner. Negative for things like returns and discounts.\n",
    "    columns.append('unitPrice') # \n",
    "    # 15.\ttotal: price times quantity. The cost of the line item. Note that this can be negative because columns.append('total') # unitPrice can be negative.     \n",
    "    columns.append('total')\n",
    "    # 16.\tregPrice: The regular price of an item. May be different from unitPrice but unitPrice plus discount columns.append('regPrice') # should be regPrice.   \n",
    "    columns.append('regPrice')\n",
    "    # 17.\taltPrice\n",
    "    columns.append('altPrice') # \n",
    "    # 18.\ttax: an indicator of whether or not the item is taxable.   \n",
    "    columns.append('tax') # \n",
    "    # 19.\ttaxexempt: mostly zero.   \n",
    "    columns.append('taxexempt') # \n",
    "    # 20.\tfoodstamp: can the item be purchased with food stamps?   \n",
    "    columns.append('foodstamp') # \n",
    "    # 21.\twicable: can the item be purchased with WIC?   \n",
    "    columns.append('wicable') # \n",
    "    # 22.\tdiscount: a marker of any discounts.    \n",
    "    columns.append('discount') # \n",
    "    # 23.\tmemDiscount: the member discounts on items.   \n",
    "    columns.append('memDiscount') # \n",
    "    # 24.\tdiscountable: beats me.   \n",
    "    columns.append('discountable') # \n",
    "    # 25.\tdiscounttype: there’s probably information in here, but I haven’t decoded it.\n",
    "    columns.append('discounttype') # \n",
    "    # 26.\tvoided: I think it’s used if an item is a void or if an item was run up and subsequently voided.   \n",
    "    columns.append('voided') # \n",
    "    # 27.\tpercentDiscount: I don’t use it.   \n",
    "    columns.append('percentDiscount') # \n",
    "    # 28.\tItemQtty: I’m not sure what this is.   \n",
    "    columns.append('ItemQtty') # \n",
    "    # 29.\tvolDiscType: Ditto   \n",
    "    columns.append('volDiscType') # \n",
    "    # 30.\tvolume: Ditto\n",
    "    columns.append('volume') # \n",
    "    # 31.\tVolSpecial: Ditto   \n",
    "    columns.append('VolSpecial') # \n",
    "    # 32.\tmixMatch: Ditto   \n",
    "    columns.append('mixMatch') # \n",
    "    # 33.\tmatched: Ditto   \n",
    "    columns.append('matched') # \n",
    "    # 34.\tmemType: Mostly NULL or 1, but I’m not sure what it signifies. Maybe institutional memberships?   \n",
    "    columns.append('memType') # \n",
    "    # 35.\tstaff: indicative of staff transactions perhaps?   \n",
    "    columns.append('staff') # \n",
    "    # 36.\tnumflag: A complicated bitflag that encodes a bunch of other information. I’ll add the communication on columns.append('numflag') # this topic to an appendix below, but it’s not critical for our purposes.   \n",
    "    columns.append('numflag')\n",
    "    # 37.\tItemstatus: Don’t know   \n",
    "    columns.append('Itemstatus') # \n",
    "    # 38.\ttenderstatus: Ditto   \n",
    "    columns.append('tenderstatus') # \n",
    "    # 39.\tcharflag: Ditto   \n",
    "    columns.append('charflag') # \n",
    "    # 40.\tvarflag: Ditto   \n",
    "    columns.append('varflag') # \n",
    "    # 41.\tbatchHeaderID: Ditto   \n",
    "    columns.append('batchHeaderID') # \n",
    "    # 42.\tlocal: is the item local?   \n",
    "    columns.append('local') # \n",
    "    # 43.\torganic: is the item organic?   \n",
    "    columns.append('organic') # \n",
    "    # 44.\tdisplay: Don’t know.   \n",
    "    columns.append('display') # \n",
    "    # 45.\treceipt: Ditto   \n",
    "    columns.append('receipt') # \n",
    "    # 46.\tcard_no: This one is important. This is the masked owner number for the transaction. It is an integer. columns.append('card_no') # If the value is 3, then the transaction is for a non-owner. You’ll find some owners (like 11572) that have a huge number of transactions. These are likely other co-ops. If you are a member of, say, the Seward Co-op you can receive discounts at the Wedge. The cashier selects your co-op and the receipt is flagged as being from that co-op.    \n",
    "    columns.append('card_no')\n",
    "    # 47.\tstore: 1 for the main store and 512 for catering.   \n",
    "    columns.append('store') # \n",
    "    # 48.\tbranch: 0 for the main store and 3 for the Wedge Table, a grab-and-go bodega they opened in January columns.append('branch') # 2015.  \n",
    "    columns.append('branch')\n",
    "    # 49.\tmatch_id: don’t know   \n",
    "    columns.append('match_id') # \n",
    "    # 50.\ttrans_id: a counter that increments the line items of a receipt.\n",
    "    columns.append('trans_id') # \n",
    "    \n",
    "    # print(columns)\n",
    "    \n",
    "    return columns\n",
    "\n",
    "\n",
    "def dtype_columns():\n",
    "    # https://www.geeksforgeeks.org/add-column-names-to-dataframe-in-pandas/\n",
    "    columns = {}\n",
    "    # 1.\tdatetime: timestamp of the transaction-row creation   \n",
    "    columns.update({'datetime':'string'}) # \n",
    "    # 2.\tregister_no: register for transaction\n",
    "    columns.update({'register_no':'string'}) # \n",
    "    # 3.\temp_no: employee number for cashier   \n",
    "    columns.update({'emp_no':'string'}) # \n",
    "    # 4.\ttrans_no: transaction number. This number counts up by day and is only unique when combined with date, columns.update({'trans_no':'string'}) # register and employee.\n",
    "    columns.update({'trans_no':'string'})\n",
    "    # 5.\tUpc: Universal Product Code for the item. 0 for non-items.\n",
    "    columns.update({'Upc':'string'}) # \n",
    "    # 6.\tdescription: product description. Includes things like Tax, Tender type, etc.   \n",
    "    columns.update({'description':'string'}) # \n",
    "    # 7.\ttrans_type: One of five values (D, G, A, T, and I). These correspond to the following types of columns.update({'trans_type':'string'}) # transactions:\n",
    "    # •\tD: Departmental rings, when the cashier just selects a department for the item.\n",
    "    # •\tG: Green patch donations. This is the donation made for shoppers who bring their own bag. \n",
    "    # •\tA: Tax\n",
    "    # •\tT: Tender, the payment row.\n",
    "    # •\tI: Items, but also includes discounts. \n",
    "    columns.update({'trans_type':'string'})\n",
    "    # 8.\ttrans_subtype: There are a lot of these. Key ones include methods of payment (CK for Check, CA for columns.update({'trans_subtype':'string'}) # Cash, CP for coupon, EF for EBT Food Stamps , WC for WIC). These are often blank for other trans_type values.\n",
    "    columns.update({'trans_subtype':'string'})\n",
    "    # 9.\ttrans_status: An important field. The field trans_status tells us more about the types transactions. columns.update({'trans_status':'string'}) # Here are the possible values:\n",
    "    # •\tBlank: The typical value.\n",
    "    # •\tM: Member discounts.\n",
    "    # •\tV: Voids \n",
    "    # •\tC: Coupons\n",
    "    # •\t0: Honestly, I think these are supposed to be blanks but they changed from 0s at some point in February 2010. \n",
    "    # •\tR: Returns.\n",
    "    # •\tJ: Juice club cards\n",
    "    columns.update({'trans_status':'string'})\n",
    "    # 10.\tdepartment: The number of the department. See the next appendix for a department lookup table.\n",
    "    columns.update({'department':'string'}) # \n",
    "    # 11.\tquantity: The purchased quantity. Beware, some items such as flowers and bulk vegetables are priced per  # cent and then sold in very large quantities (like 1000 for a $10 bouquet.)   \n",
    "    columns.update({'quantity':'string'})\n",
    "    # 12.\tScale: The reading on the scale. Note that the capital here is not a typo. This is one field that \n",
    "    columns.update({'Scale':'string'}) # weirdly has a capital first letter.    \n",
    "    # 13.\tcost: the per-unit cost of an item to the Wedge. This is not uniformly populated. \n",
    "    columns.update({'cost':'string'}) # \n",
    "    # 14.\tunitPrice:  the per-unit cost of an item to an owner. Negative for things like returns and discounts.\n",
    "    columns.update({'unitPrice':'string'}) # \n",
    "    # 15.\ttotal: price times quantity. The cost of the line item. Note that this can be negative because columns.update({'total':'string'}) # unitPrice can be negative.     \n",
    "    columns.update({'total':'string'})\n",
    "    # 16.\tregPrice: The regular price of an item. May be different from unitPrice but unitPrice plus discount columns.update({'regPrice':'string'}) # should be regPrice.   \n",
    "    columns.update({'regPrice':'string'})\n",
    "    # 17.\taltPrice\n",
    "    columns.update({'altPrice':'string'}) # \n",
    "    # 18.\ttax: an indicator of whether or not the item is taxable.   \n",
    "    columns.update({'tax':'string'}) # \n",
    "    # 19.\ttaxexempt: mostly zero.   \n",
    "    columns.update({'taxexempt':'string'}) # \n",
    "    # 20.\tfoodstamp: can the item be purchased with food stamps?   \n",
    "    columns.update({'foodstamp':'string'}) # \n",
    "    # 21.\twicable: can the item be purchased with WIC?   \n",
    "    columns.update({'wicable':'string'}) # \n",
    "    # 22.\tdiscount: a marker of any discounts.    \n",
    "    columns.update({'discount':'string'}) # \n",
    "    # 23.\tmemDiscount: the member discounts on items.   \n",
    "    columns.update({'memDiscount':'string'}) # \n",
    "    # 24.\tdiscountable: beats me.   \n",
    "    columns.update({'discountable':'string'}) # \n",
    "    # 25.\tdiscounttype: there’s probably information in here, but I haven’t decoded it.\n",
    "    columns.update({'discounttype':'string'}) # \n",
    "    # 26.\tvoided: I think it’s used if an item is a void or if an item was run up and subsequently voided.   \n",
    "    columns.update({'voided':'string'}) # \n",
    "    # 27.\tpercentDiscount: I don’t use it.   \n",
    "    columns.update({'percentDiscount':'string'}) # \n",
    "    # 28.\tItemQtty: I’m not sure what this is.   \n",
    "    columns.update({'ItemQtty':'string'}) # \n",
    "    # 29.\tvolDiscType: Ditto   \n",
    "    columns.update({'volDiscType':'string'}) # \n",
    "    # 30.\tvolume: Ditto\n",
    "    columns.update({'volume':'string'}) # \n",
    "    # 31.\tVolSpecial: Ditto   \n",
    "    columns.update({'VolSpecial':'string'}) # \n",
    "    # 32.\tmixMatch: Ditto   \n",
    "    columns.update({'mixMatch':'string'}) # \n",
    "    # 33.\tmatched: Ditto   \n",
    "    columns.update({'matched':'string'}) # \n",
    "    # 34.\tmemType: Mostly NULL or 1, but I’m not sure what it signifies. Maybe institutional memberships?   \n",
    "    columns.update({'memType':'string'}) # \n",
    "    # 35.\tstaff: indicative of staff transactions perhaps?   \n",
    "    columns.update({'staff':'string'}) # \n",
    "    # 36.\tnumflag: A complicated bitflag that encodes a bunch of other information. I’ll add the communication on columns.update({'numflag':'string'}) # this topic to an appendix below, but it’s not critical for our purposes.   \n",
    "    columns.update({'numflag':'string'})\n",
    "    # 37.\tItemstatus: Don’t know   \n",
    "    columns.update({'Itemstatus':'string'}) # \n",
    "    # 38.\ttenderstatus: Ditto   \n",
    "    columns.update({'tenderstatus':'string'}) # \n",
    "    # 39.\tcharflag: Ditto   \n",
    "    columns.update({'charflag':'string'}) # \n",
    "    # 40.\tvarflag: Ditto   \n",
    "    columns.update({'varflag':'string'}) # \n",
    "    # 41.\tbatchHeaderID: Ditto   \n",
    "    columns.update({'batchHeaderID':'string'}) # \n",
    "    # 42.\tlocal: is the item local?   \n",
    "    columns.update({'local':'string'}) # \n",
    "    # 43.\torganic: is the item organic?   \n",
    "    columns.update({'organic':'string'}) # \n",
    "    # 44.\tdisplay: Don’t know.   \n",
    "    columns.update({'display':'string'}) # \n",
    "    # 45.\treceipt: Ditto   \n",
    "    columns.update({'receipt':'string'}) # \n",
    "    # 46.\tcard_no: This one is important. This is the masked owner number for the transaction. It is an integer. If the value is 3, then the transaction is for a non-owner. You’ll find some owners (like 11572) that have a huge number of transactions. These are likely other co-ops. If you are a member of, say, the Seward Co-op you can receive discounts at the Wedge. The cashier selects your co-op and the receipt is flagged as being from that co-op.    \n",
    "    columns.update({'card_no':'string'})\n",
    "    # 47.\tstore: 1 for the main store and 512 for catering.   \n",
    "    columns.update({'store':'string'}) # \n",
    "    # 48.\tbranch: 0 for the main store and 3 for the Wedge Table, a grab-and-go bodega they opened in January 2015.  \n",
    "    columns.update({'branch':'string'})\n",
    "    # 49.\tmatch_id: don’t know   \n",
    "    columns.update({'match_id':'string'}) # \n",
    "    # 50.\ttrans_id: a counter that increments the line items of a receipt.\n",
    "    columns.update({'trans_id':'string'}) # \n",
    "    \n",
    "    # print(columns)\n",
    "\n",
    "    return columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19134b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8378062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d0c003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bmkt670-fall2022-wedge-project:wedgedataset\n"
     ]
    }
   ],
   "source": [
    "# These first two values will be different on your machine. \n",
    "# service_path = \"/Users/chandler/Dropbox/Teaching/\"\n",
    "# service_file = 'umt-msba-037daf11ee16.json' # change this to your authentication information  \n",
    "# gbq_proj_id = 'umt-msba' # change this to your project. \n",
    "service_path = \"/home/blackvwgolf95/\"\n",
    "service_file = 'bmkt670-fall2022-wedge-project-58c9c55deb49.json' # change this to your authentication information  \n",
    "gbq_proj_id = 'bmkt670-fall2022-wedge-project' # change this to your project. \n",
    "dataset_id = 'wedgedataset'\n",
    "\n",
    "# And this should stay the same. \n",
    "private_key =service_path + service_file\n",
    "\n",
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "\n",
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)\n",
    "\n",
    "for item in client.list_datasets() : \n",
    "    print(item.full_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e8344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a8dd9f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a6f5517",
   "metadata": {},
   "source": [
    "### Checking for and deleting monthly tables\n",
    "\n",
    "We'll get all the tables in our wedge data set that match our pattern, then delete them. We do not want to accidentally delete the item lookup table that we put in this data set in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c7820d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a regex that matches our table pattern\n",
    "ymd_pattern = re.compile(r\"^wedge_\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a33db119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "tables = client.list_tables(dataset_id)  \n",
    "\n",
    "for table in tables:\n",
    "    \n",
    "    print(f'Looking at {table.table_id}\\n')\n",
    "\n",
    "    # Test to see if table.table_id matches the pattern\n",
    "    # if so, delete it\n",
    "    if ymd_pattern.match(table.table_id):\n",
    "        # print(table.table_id)\n",
    "        print(f'She swiped right, we have a MATCH! {table.table_id}\\n')\n",
    "        table_id = \".\".join([gbq_proj_id,dataset_id,table.table_id])\n",
    "        client.delete_table(table_id, not_found_ok=True)\n",
    "        print(f\"She blocked us, all hope is lost {table_id}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cd6bb",
   "metadata": {},
   "source": [
    "## Phase 1, Upload Clean Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd09464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all the files now...\n",
      "File Exists, skipping\n",
      "holder/transArchive_201001_201003_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201004_201006_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201007_201009_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201010_201012_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201101_201103_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201104_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201105_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201106_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201107_201109_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201110_201112_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201201_201203_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201201_201203_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201204_201206_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201204_201206_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201207_201209_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201207_201209_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201210_201212_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201210_201212_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201301_201303_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201301_201303_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201304_201306_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201304_201306_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201307_201309_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201307_201309_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201310_201312_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201310_201312_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201401_201403_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201401_201403_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201404_201406_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201404_201406_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201407_201409_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201407_201409_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201410_201412_inactive_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201410_201412_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201501_201503_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201504_201506_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201507_201509_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201510_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201511_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201512_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201601_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201602_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201603_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201604_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201605_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201606_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201607_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201608_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201609_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201610_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201611_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201612_small.zip\n",
      "File Exists, skipping\n",
      "holder/transArchive_201701_small.zip\n",
      "Done Extracting!\n",
      "File:  holder/transArchive_201001_201003_small.zip  Size: 2949088\n",
      "File:  holder/transArchive_201004_201006_small.zip  Size: 2910241\n",
      "File:  holder/transArchive_201007_201009_small.zip  Size: 2908263\n",
      "File:  holder/transArchive_201010_201012_small.zip  Size: 2936935\n",
      "File:  holder/transArchive_201101_201103_small.zip  Size: 2970266\n",
      "File:  holder/transArchive_201104_small.zip  Size: 2147921\n",
      "File:  holder/transArchive_201105_small.zip  Size: 2163420\n",
      "File:  holder/transArchive_201106_small.zip  Size: 2133072\n",
      "File:  holder/transArchive_201107_201109_small.zip  Size: 2960818\n",
      "File:  holder/transArchive_201110_201112_small.zip  Size: 2972532\n",
      "File:  holder/transArchive_201201_201203_small.zip  Size: 2966105\n",
      "File:  holder/transArchive_201204_201206_small.zip  Size: 2970529\n",
      "File:  holder/transArchive_201207_201209_small.zip  Size: 2969220\n",
      "File:  holder/transArchive_201210_201212_small.zip  Size: 2955834\n",
      "File:  holder/transArchive_201301_201303_small.zip  Size: 2959095\n",
      "File:  holder/transArchive_201304_201306_small.zip  Size: 2955089\n",
      "File:  holder/transArchive_201307_201309_small.zip  Size: 2957544\n",
      "File:  holder/transArchive_201310_201312_small.zip  Size: 2950052\n",
      "File:  holder/transArchive_201401_201403_small.zip  Size: 2989703\n",
      "File:  holder/transArchive_201404_201406_small.zip  Size: 2942862\n",
      "File:  holder/transArchive_201407_201409_small.zip  Size: 2961548\n",
      "File:  holder/transArchive_201410_201412_small.zip  Size: 2963076\n",
      "File:  holder/transArchive_201501_201503_small.zip  Size: 2930140\n",
      "File:  holder/transArchive_201504_201506_small.zip  Size: 2882272\n",
      "File:  holder/transArchive_201507_201509_small.zip  Size: 2895394\n",
      "File:  holder/transArchive_201510_small.zip  Size: 2899004\n",
      "File:  holder/transArchive_201511_small.zip  Size: 1966097\n",
      "File:  holder/transArchive_201512_small.zip  Size: 1828014\n",
      "File:  holder/transArchive_201601_small.zip  Size: 1969606\n",
      "File:  holder/transArchive_201602_small.zip  Size: 1948160\n",
      "File:  holder/transArchive_201603_small.zip  Size: 1936746\n",
      "File:  holder/transArchive_201604_small.zip  Size: 1946085\n",
      "File:  holder/transArchive_201605_small.zip  Size: 1940801\n",
      "File:  holder/transArchive_201606_small.zip  Size: 1924059\n",
      "File:  holder/transArchive_201607_small.zip  Size: 1930991\n",
      "File:  holder/transArchive_201608_small.zip  Size: 1927607\n",
      "File:  holder/transArchive_201609_small.zip  Size: 1921693\n",
      "File:  holder/transArchive_201610_small.zip  Size: 1938891\n",
      "File:  holder/transArchive_201611_small.zip  Size: 1923868\n",
      "File:  holder/transArchive_201612_small.zip  Size: 1926246\n",
      "File:  holder/transArchive_201701_small.zip  Size: 1948645\n",
      "Done building file list\n"
     ]
    }
   ],
   "source": [
    "# In this cell, do the following: \n",
    "\n",
    "# Master list of all data files\n",
    "data_files = []\n",
    "\n",
    "from zipfile import ZipFile # usually you'd do all these imports at the beginning\n",
    "\n",
    "with ZipFile( zip_file_name, 'r') as zf : \n",
    "    # printing what's in the zip file.  \n",
    "    # zf.printdir() \n",
    "  \n",
    "    # extracting all the files \n",
    "    print('Extracting all the files now...') \n",
    "    # pick a folder name already in .gitignore\n",
    "    \n",
    "    # Instead of always extracting ALL, check if file exists first\n",
    "    # zf.extractall(working_directory) \n",
    "    \n",
    "    zipped_files = zf.namelist()\n",
    "    # display_zip_contents(zipped_files)\n",
    "    \n",
    "    # Only extract files if they don't exist\n",
    "    for file_name in zipped_files :\n",
    "        \n",
    "        # Ignore __MACOSX hidden files\n",
    "        if(file_name.startswith( '__' )):\n",
    "            continue\n",
    "        \n",
    "        # Ignore folders\n",
    "        if(file_name.endswith( '/' )):\n",
    "            continue\n",
    "        \n",
    "        # Ignore inactive files\n",
    "        if(file_name.endswith( '_inactive_clean.csv' )):\n",
    "            continue\n",
    "        \n",
    "        if os.path.exists(working_directory + file_name) :\n",
    "            print(\"File Exists, skipping\")\n",
    "            print(file_name)\n",
    "        else :\n",
    "            print(\"Need to Extract\")\n",
    "            print(file_name)\n",
    "            zf.extract(file_name, working_directory) \n",
    "        \n",
    "        # data_files.append(file_name)\n",
    "            \n",
    "    \n",
    "    print('Done Extracting!')\n",
    "    # print(zf.namelist())\n",
    "    # zipped_files = zf.namelist()\n",
    "    # display_zip_contents(zipped_files)\n",
    "    \n",
    "    for file_name in zipped_files :\n",
    "        # Ignore __MACOSX hidden files\n",
    "        if(file_name.startswith( '__' )):\n",
    "            continue\n",
    "        # Ignore folders\n",
    "        if(file_name.endswith( '/' )):\n",
    "            continue\n",
    "        \n",
    "        # Ignore inactive files\n",
    "        if(file_name.endswith( '_inactive_small.zip' )):\n",
    "            continue\n",
    "\n",
    "        print(\"File: \", file_name,\" Size:\", os.path.getsize(working_directory+file_name))\n",
    "        with ZipFile(working_directory+file_name, 'r') as zftmp :\n",
    "            zftmp.extractall(working_directory)\n",
    "            tmp_zipped_files = zftmp.namelist()\n",
    "            # display_zip_contents(tmp_zipped_files)\n",
    "            for tmp_file_name in tmp_zipped_files :\n",
    "                data_files.append(tmp_file_name)\n",
    "\n",
    "print(\"Done building file list\")\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3d70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "434fea76",
   "metadata": {},
   "source": [
    "## Verify csv Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdc8d8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:  transArchive_201001_201003_small.csv  Size: 2948918\n",
      "File:  transArchive_201004_201006_small.csv  Size: 2910071\n",
      "File:  transArchive_201007_201009_small.csv  Size: 2908093\n",
      "File:  transArchive_201010_201012_small.csv  Size: 2936765\n",
      "File:  transArchive_201101_201103_small.csv  Size: 2970096\n",
      "File:  transArchive_201104_small.csv  Size: 2147765\n",
      "File:  transArchive_201105_small.csv  Size: 2163264\n",
      "File:  transArchive_201106_small.csv  Size: 2132916\n",
      "File:  transArchive_201107_201109_small.csv  Size: 2960648\n",
      "File:  transArchive_201110_201112_small.csv  Size: 2972362\n",
      "File:  transArchive_201201_201203_small.csv  Size: 2965935\n",
      "File:  transArchive_201204_201206_small.csv  Size: 2970359\n",
      "File:  transArchive_201207_201209_small.csv  Size: 2969050\n",
      "File:  transArchive_201210_201212_small.csv  Size: 2955664\n",
      "File:  transArchive_201301_201303_small.csv  Size: 2958925\n",
      "File:  transArchive_201304_201306_small.csv  Size: 2954919\n",
      "File:  transArchive_201307_201309_small.csv  Size: 2957374\n",
      "File:  transArchive_201310_201312_small.csv  Size: 2949882\n",
      "File:  transArchive_201401_201403_small.csv  Size: 2989533\n",
      "File:  transArchive_201404_201406_small.csv  Size: 2942692\n",
      "File:  transArchive_201407_201409_small.csv  Size: 2961378\n",
      "File:  transArchive_201410_201412_small.csv  Size: 2962906\n",
      "File:  transArchive_201501_201503_small.csv  Size: 2929970\n",
      "File:  transArchive_201504_201506_small.csv  Size: 2882102\n",
      "File:  transArchive_201507_201509_small.csv  Size: 2895224\n",
      "File:  transArchive_201510_small.csv  Size: 2898848\n",
      "File:  transArchive_201511_small.csv  Size: 1965941\n",
      "File:  transArchive_201512_small.csv  Size: 1827858\n",
      "File:  transArchive_201601_small.csv  Size: 1969450\n",
      "File:  transArchive_201602_small.csv  Size: 1948004\n",
      "File:  transArchive_201603_small.csv  Size: 1936590\n",
      "File:  transArchive_201604_small.csv  Size: 1945929\n",
      "File:  transArchive_201605_small.csv  Size: 1940645\n",
      "File:  transArchive_201606_small.csv  Size: 1923903\n",
      "File:  transArchive_201607_small.csv  Size: 1930835\n",
      "File:  transArchive_201608_small.csv  Size: 1927451\n",
      "File:  transArchive_201609_small.csv  Size: 1921537\n",
      "File:  transArchive_201610_small.csv  Size: 1938735\n",
      "File:  transArchive_201611_small.csv  Size: 1923712\n",
      "File:  transArchive_201612_small.csv  Size: 1926090\n",
      "File:  transArchive_201701_small.csv  Size: 1948489\n"
     ]
    }
   ],
   "source": [
    "# print(data_files)\n",
    "display_file_contents(data_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c96111",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3119c333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Uploads!\n"
     ]
    }
   ],
   "source": [
    "# 1. Read in the items files one at a time.\n",
    "for file_name in data_files :\n",
    "    \n",
    "    print(file_name)\n",
    " \n",
    "    # https://stackoverflow.com/a/27232309\n",
    "    transactions = pd.read_csv(working_directory+file_name, \n",
    "                               header=None, \n",
    "                               names=data_columns(), \n",
    "                               dtype=dtype_columns()\n",
    "                              ) # \n",
    "  \n",
    "\n",
    "    # Construct table name from index\n",
    "    table_name = \"wedge_\"+file_name.replace(\".\",\"-\").replace(\"/\",\"-\")\n",
    "    # print(type(name))\n",
    "\n",
    "    # 3. For each month in the file, subset the data to that month and \n",
    "    #    upload the data to a table called `dram_items_YYYYMM01`. \n",
    "    table_id = \".\".join([gbq_proj_id,dataset_id,table_name])\n",
    "    # print(table_id)\n",
    "    pandas_gbq.to_gbq(transactions, table_id, project_id=gbq_proj_id,if_exists=\"replace\") # let's discuss this last bit\n",
    "\n",
    "print(\"Completed Uploads!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f79e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uploading..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbcdfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4467e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to illustrate the concept\n",
    "# of threading\n",
    "# importing the threading module\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def upload_file(file_name):\n",
    "    # https://stackoverflow.com/a/27232309\n",
    "    transactions = pd.read_csv(working_directory+file_name, \n",
    "                               header=None, \n",
    "                               names=data_columns(), \n",
    "                               dtype=dtype_columns()\n",
    "                              ) # \n",
    "\n",
    "    # Construct table name from index\n",
    "    table_name = \"wedge_\"+file_name.replace(\".\",\"-\").replace(\"/\",\"-\")\n",
    "    # print(type(name))\n",
    "\n",
    "    # 3. For each month in the file, subset the data to that month and \n",
    "    #    upload the data to a table called `dram_items_YYYYMM01`. \n",
    "    table_id = \".\".join([gbq_proj_id,dataset_id,table_name])\n",
    "    \n",
    "    # print(table_id)\n",
    "    \n",
    "    job = client.load_table_from_dataframe(\n",
    "        transactions, table_id, job_config=job_config\n",
    "    ) # \n",
    "\n",
    "    # Wait for the load job to complete. (I omit this step)\n",
    "    print(job.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a645f4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transArchive_201001_201003_small.csv\n",
      "transArchive_201004_201006_small.csv\n",
      "transArchive_201007_201009_small.csv\n",
      "transArchive_201010_201012_small.csv\n",
      "transArchive_201101_201103_small.csv\n",
      "transArchive_201104_small.csv\n",
      "transArchive_201105_small.csv\n",
      "transArchive_201106_small.csv\n",
      "transArchive_201107_201109_small.csv\n",
      "transArchive_201110_201112_small.csv\n",
      "transArchive_201201_201203_small.csv\n",
      "transArchive_201204_201206_small.csv\n",
      "transArchive_201207_201209_small.csv\n",
      "transArchive_201210_201212_small.csv\n",
      "transArchive_201301_201303_small.csv\n",
      "transArchive_201304_201306_small.csv\n",
      "transArchive_201307_201309_small.csv\n",
      "transArchive_201310_201312_small.csv\n",
      "transArchive_201401_201403_small.csv\n",
      "transArchive_201404_201406_small.csv\n",
      "transArchive_201407_201409_small.csv\n",
      "transArchive_201410_201412_small.csv\n",
      "transArchive_201501_201503_small.csv\n",
      "transArchive_201504_201506_small.csv\n",
      "transArchive_201507_201509_small.csv\n",
      "transArchive_201510_small.csv\n",
      "transArchive_201511_small.csv\n",
      "transArchive_201512_small.csv\n",
      "transArchive_201601_small.csv\n",
      "transArchive_201602_small.csv\n",
      "transArchive_201603_small.csv\n",
      "transArchive_201604_small.csv\n",
      "transArchive_201605_small.csv\n",
      "transArchive_201606_small.csv\n",
      "transArchive_201607_small.csv\n",
      "transArchive_201608_small.csv\n",
      "transArchive_201609_small.csv\n",
      "transArchive_201610_small.csv\n",
      "transArchive_201611_small.csv\n",
      "transArchive_201612_small.csv\n",
      "transArchive_201701_small.csv\n",
      "Done!\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=5f080bd1-e806-4e6e-b18d-dc232b73e78a>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=c347b19e-11b8-400d-8308-ce8a0303428d>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=8e48bcd6-55fb-45bd-ad88-299880e8f0e0>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=cfeb29d8-4720-4294-b59a-675e951c8610>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=6aacc457-6c62-460d-8e57-875f8d1165c2>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=da185a03-8793-446f-9c16-abb1ac2fe30a>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=980d1eaf-3bf2-4e4f-ba7f-6788ebfd2f38>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=5184f00c-ce81-4f80-b309-da0c00d85b3b>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=230a60c6-f912-4bed-ab17-930e726f60e8>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=6bc9faed-4269-4503-a377-2a123d25a09c>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=17987526-54c0-4ddf-b52e-40a827fd7ddc>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=f904f568-46dd-437e-b9b0-f56d6c88368e>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=c612b587-3e25-43ed-a628-48d55285bf1f>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=f2d84d53-46fc-4b0d-8728-a67d0a0255a1>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=06bbec71-d1b3-4465-87a9-da2f3da7f4b2>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=1c4123d3-a652-4b6a-aeeb-5ce45bc87535>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=06af8bd6-82a4-4544-9b16-cbc9a25cf4d3>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=bf846a4a-b173-43fc-893b-d08e0d71ddac>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=5e68ed9b-37f6-478a-85a7-09f044d890fc>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=dae5b81a-e981-460b-83d7-b5e782471d7d>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=79acd4c4-3e43-4e42-8865-7941f027f3f1>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=2d046395-5ca3-4558-a7a4-ae88fe93e37e>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=ee0ee1c3-4ea4-4430-877b-f365056bade3>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=0f7405f3-3dab-417f-ac79-81ded0ecab13>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=e86d48b6-9c88-4458-b30c-1517c05448e4>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=758482a0-329c-4797-8795-4c266de3c0c2>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=8c82d86d-422c-478a-8f1d-8c7094f44ce7>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=d4bc1ea7-24f6-4d88-87a8-57b443bfb773>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=a4ddb44c-b12d-4f8a-b6ea-5b22bab3364f>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=e4ce83ed-613a-47e3-90a1-acace11d87f9>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=2e52d512-d420-46ca-a3bd-2ce52f9e20a3>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=2abbc495-7729-430a-bbf4-69b0cf427535>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=ff7cd011-8a22-4e26-ba22-013969df7d6e>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=898c9807-3b33-4460-b234-7725d92f27aa>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=fcb223b1-81ff-4327-89c2-bc8aabea8ddd>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=c358f162-1d5d-45e4-b995-6d983ed48527>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=ae1cf81c-8f30-45e2-8380-8acbfc8935e2>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=b3ccfd6f-52fd-4355-9814-c9b7242b4aa1>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=7019d6c6-7b95-4052-b5d2-a457870420cc>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=16d4f9b2-8759-4b55-b21b-7ebdb90a321d>\n",
      "LoadJob<project=bmkt670-fall2022-wedge-project, location=US, id=288a2d4d-4b17-4ef4-8bdf-22a5bde20de0>\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/a/68806012\n",
    "\n",
    "# Since string columns use the \"object\" dtype, pass in a (partial) schema\n",
    "# to ensure the correct BigQuery data type.\n",
    "# job_config = bigquery.LoadJobConfig(schema=[\n",
    "#     dtype_columns(),\n",
    "# ])\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\",)\n",
    "\n",
    "# 1. Read in the items files one at a time.\n",
    "for file_name in data_files :\n",
    "    print(file_name)\n",
    "    # upload_file(file_name)\n",
    "    threading.Thread(target=upload_file, args=(file_name,)).start()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ebc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3074a732",
   "metadata": {},
   "source": [
    "## Phase 2, Clean & Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeada6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file_name in data_files :\n",
    "#     process_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db770074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in the items files one at a time.\n",
    "    for file_name in zipped_files :\n",
    "        # Ignore __MACOSX hidden files\n",
    "        if(file_name.startswith( '__' )):\n",
    "            continue\n",
    "        \n",
    "        # print(file_name)\n",
    "        item_lu = ( pd.read_csv(\"./eggs/\"+file_name, low_memory=False)\n",
    "           #.remove_empty()\n",
    "           #.dropna()\n",
    "           #.clean_names() # ,sep=\"\\t\"\n",
    "          )\n",
    "        # 2. Do the same cleaning we did in Part 1 (clean names, \n",
    "        #    make sku an empty string, fix dollars, make modifiers_applied a string)\n",
    "\n",
    "        # Clean the names with the janitor package.\n",
    "        item_lu = janitor.clean_names(item_lu)\n",
    "\n",
    "        for column in ( 'gross_sales', 'discounts', 'net_sales', 'tax' ):\n",
    "            # Convert the fields that have dollar signs (such as `gross_sales`) into numeric data. Watch out for dollar signs and commas.\n",
    "            item_lu[column] = ( item_lu[column]\n",
    "                               .str.replace(\"$\", '', regex=False)\n",
    "                               .str.replace(\",\", '', regex=False)\n",
    "                               .astype(float) )\n",
    "\n",
    "        # Change the type of the column `modifiers_applied` to string.\n",
    "        item_lu['modifiers_applied'] = item_lu['modifiers_applied'].astype(str)\n",
    "\n",
    "        # Replace the `sku` column with a column of empty strings. \n",
    "        item_lu['sku'] = ''\n",
    "        \n",
    "        # print( item_lu.head() )\n",
    "        item_lu.index = pd.to_datetime(item_lu['date']) # ,format='%y-%m-%d'\n",
    "\n",
    "        # https://stackoverflow.com/a/24083253\n",
    "        grouped = item_lu.groupby(pd.Grouper(freq='M'))\n",
    "        for name, group in grouped:\n",
    "            # print(name)\n",
    "            # print(len(group))\n",
    "            \n",
    "            # Construct table name from index\n",
    "            table_name = \"dram_items_\"+reformat_date(name.strftime('%Y-%m-%d'))\n",
    "            # print(type(name))\n",
    "            \n",
    "            # 3. For each month in the file, subset the data to that month and \n",
    "            #    upload the data to a table called `dram_items_YYYYMM01`. \n",
    "            table_id = \".\".join([gbq_proj_id,dataset_id,table_name])\n",
    "            # print(table_id)\n",
    "            pandas_gbq.to_gbq(item_lu, table_id, project_id=gbq_proj_id,if_exists=\"replace\") # let's discuss this last bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019959e",
   "metadata": {},
   "source": [
    "# Cleanup ALL Local Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e37cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://linuxize.com/post/python-delete-files-and-directories/\n",
    "try:\n",
    "    shutil.rmtree(working_directory)\n",
    "    print('Done Cleanup')\n",
    "    print(\"Completed Exit Code 0\")\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (working_directory, e.strerror))\n",
    "    print(\"Completed Exit Code -1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
